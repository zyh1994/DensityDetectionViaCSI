# Authors

* Jiaqi Geng jiaqigen@andrew.cmu.edu Carnegie Mellon University Pittsburgh, PA, USA
* Dong Huang donghuang@cmu.edu Carnegie Mellon University Pittsburgh, PA, USA
* Fernando De la Torre ftorre@cs.cmu.edu Carnegie Mellon University Pittsburgh, PA, USA

# Bibtex
Geng J, Huang D, De la Torre F. DensePose From WiFi[J]. arXiv preprint arXiv:2301.00250, 2022.

# 0. ABSTRACT

Advances in computer vision and machine learning techniques have led to significant development in 2D and 3D human pose estimation from RGB cameras, LiDAR, and radars. However, human pose esti- mation from images is adversely affected by occlusion and lighting, which are common in many scenarios of interest. Radar and LiDAR technologies, on the other hand, need specialized hardware that is expensive and power-intensive. Furthermore, placing these sensors in non-public areas raises significant privacy concerns.

> è®¡ç®—æœºè§†è§‰å’Œæœºå™¨å­¦ä¹ æŠ€æœ¯çš„è¿›æ­¥å·²ç»å¯¼è‡´RGBæ‘„åƒå¤´ã€LiDARå’Œé›·è¾¾ä¸­2Då’Œ3Däººä½“å§¿åŠ¿ä¼°è®¡çš„é‡å¤§å‘å±•ã€‚ç„¶è€Œï¼Œå›¾åƒä¸­çš„äººä½“å§¿åŠ¿ä¼°è®¡å—åˆ°é®æŒ¡å’Œç…§æ˜çš„ä¸åˆ©å½±å“ï¼Œè¿™åœ¨è®¸å¤šå…³æ³¨çš„åœºæ™¯ä¸­æ˜¯å¸¸è§çš„ã€‚å¦ä¸€æ–¹é¢ï¼Œé›·è¾¾å’ŒLiDARæŠ€æœ¯éœ€è¦ä¸“é—¨çš„ç¡¬ä»¶ï¼Œä»·æ ¼æ˜‚è´µï¼Œè€—èƒ½å¤§ã€‚æ­¤å¤–ï¼Œåœ¨éå…¬å…±åŒºåŸŸæ”¾ç½®è¿™äº›ä¼ æ„Ÿå™¨ä¼šå¼•å‘ä¸¥é‡çš„éšç§é—®é¢˜ã€‚

To address these limitations, recent research has explored the use of WiFi antennas (1D sensors) for body segmentation and key-point body detection. This paper further expands on the use of the WiFi signal in combination with deep learning architectures, commonly used in computer vision, to estimate dense human pose correspon- dence. We developed a deep neural network that maps the phase and amplitude of WiFi signals to UV coordinates within 24 human regions. The results of the study reveal that our model can estimate the dense pose of multiple subjects, with comparable performance to image-based approaches, by utilizing WiFi signals as the only input. This paves the way for low-cost, broadly accessible, and privacy-preserving algorithms for human sensing.

> ä¸ºäº†è§£å†³è¿™äº›å±€é™ï¼Œæœ€è¿‘çš„ç ”ç©¶å·²ç»æ¢ç´¢äº†ä½¿ç”¨WiFiå¤©çº¿ï¼ˆ1Dä¼ æ„Ÿå™¨ï¼‰è¿›è¡Œèº«ä½“åˆ†å‰²å’Œå…³é”®ç‚¹èº«ä½“æ£€æµ‹ã€‚æœ¬æ–‡è¿›ä¸€æ­¥æ‹“å±•äº†WiFiä¿¡å·ä¸è®¡ç®—æœºè§†è§‰ä¸­å¸¸ç”¨çš„æ·±åº¦å­¦ä¹ ä½“ç³»ç»“æ„çš„ç»“åˆï¼Œä»¥ä¼°è®¡å¯†é›†çš„äººä½“å§¿åŠ¿å¯¹åº”å…³ç³»ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ç§æ·±åº¦ç¥ç»ç½‘ç»œï¼Œå°†WiFiä¿¡å·çš„ç›¸ä½å’ŒæŒ¯å¹…æ˜ å°„åˆ°24ä¸ªäººä½“åŒºåŸŸå†…çš„UVåæ ‡ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å¯ä»¥åˆ©ç”¨WiFiä¿¡å·ä½œä¸ºå”¯ä¸€è¾“å…¥ï¼Œä¼°è®¡å¤šä¸ªå¯¹è±¡çš„å¯†é›†å§¿åŠ¿ï¼Œä¸åŸºäºå›¾åƒçš„æ–¹æ³•å…·æœ‰ç›¸å½“çš„æ€§èƒ½ã€‚è¿™ä¸ºä½æˆæœ¬ã€å¹¿æ³›å¯è®¿é—®å’Œéšç§ä¿æŠ¤çš„äººä½“ä¼ æ„Ÿç®—æ³•é“ºå¹³äº†é“è·¯ã€‚

**Keywords**

Pose Estimation, Dense Body Pose Estimation, WiFi Signals, Key- point Estimation, Body Segmentation, Object Detection, UV Coor- dinates, Phase and Amplitude, Phase Sanitization, Channel State Information, Domain Translation, Deep Neural Network, Mask- RCNN


# 1. INTRODUCTION

Much progress has been made in human pose estimation using 2D [7, 8, 12, 22, 28, 33] and 3D [17, 32] sensors in the last few years (e.g., RGB sensors, LiDARs, radars), fueled by applications in autonomous driving and augmented reality. These traditional sensors, however, are constrained by both technical and practical considerations. LiDAR and radar sensors are frequently seen as being out of reach for the average household or small business due to their high cost. For example, the medium price of one of the most common COTS LiDAR, Intel L515, is around 700 dollars, and the prices for ordinary radar detectors range from 200 dollars to 600 dollars. In addition, these sensors are too power-consuming for daily and household use. As for RGB cameras, narrow field of view and poor lighting conditions, such as glare and darkness, can have a severe impact on camera-based approaches. Occlusion is another obstacle that prevents the camera-based model from generating reasonable pose predictions in images. This is especially worrisome for indoors scenarios, where furniture typically occludes people.

> è¿‘å¹´æ¥ï¼Œäººä½“å§¿åŠ¿ä¼°è®¡åœ¨2D [7, 8, 12, 22, 28, 33] å’Œ3D [17, 32]ä¼ æ„Ÿå™¨æ–¹é¢å–å¾—äº†å¾ˆå¤§è¿›å±•ï¼ˆä¾‹å¦‚ï¼ŒRGBä¼ æ„Ÿå™¨ï¼ŒLiDARï¼Œé›·è¾¾ï¼‰ï¼Œå¾—ç›Šäºè‡ªåŠ¨é©¾é©¶å’Œå¢å¼ºç°å®åº”ç”¨ã€‚ç„¶è€Œï¼Œè¿™äº›ä¼ ç»Ÿä¼ æ„Ÿå™¨å—åˆ°æŠ€æœ¯å’Œå®é™…é™åˆ¶çš„é™åˆ¶ã€‚LiDARå’Œé›·è¾¾ä¼ æ„Ÿå™¨å› å…¶é«˜æˆæœ¬å¸¸å¸¸è¢«è®¤ä¸ºä¸é€‚åˆæ™®é€šå®¶åº­æˆ–å°ä¼ä¸šä½¿ç”¨ã€‚ä¾‹å¦‚ï¼Œæœ€å¸¸è§çš„COTS LiDARä¹‹ä¸€Intel L515çš„ä¸­ç­‰ä»·æ ¼çº¦ä¸º700ç¾å…ƒï¼Œè€Œæ™®é€šé›·è¾¾æ¢æµ‹å™¨çš„ä»·æ ¼ä»200ç¾å…ƒåˆ°600ç¾å…ƒä¸ç­‰ã€‚æ­¤å¤–ï¼Œè¿™äº›ä¼ æ„Ÿå™¨å¯¹äºæ—¥å¸¸å’Œå®¶åº­ä½¿ç”¨æ¥è¯´å¤ªè€—ç”µã€‚è‡³äºRGBç›¸æœºï¼Œç‹­çª„çš„è§†é‡å’Œè¾ƒå·®çš„ç…§æ˜æ¡ä»¶ï¼Œä¾‹å¦‚çœ©å…‰å’Œé»‘æš—ï¼Œå¯èƒ½å¯¹åŸºäºç›¸æœºçš„æ–¹æ³•äº§ç”Ÿä¸¥é‡å½±å“ã€‚é®æŒ¡æ˜¯å¦ä¸€ä¸ªé˜»ç¢ç›¸æœºæ¨¡å‹åœ¨å›¾åƒä¸­ç”Ÿæˆåˆç†å§¿åŠ¿é¢„æµ‹çš„éšœç¢ã€‚è¿™å¯¹å®¤å†…åœºæ™¯å°¤å…¶ä»¤äººæ‹…å¿§ï¼Œå› ä¸ºå®¶å…·é€šå¸¸ä¼šé®æŒ¡äººã€‚

More importantly, privacy concerns prevent the use of these technologies in non-public places. For instance, most people are uncomfortable with having cameras recording them in their homes, and in certain areas (such as the bathroom) it will not be feasible to install them. These issues are particularly critical in healthcare applications, that are increasingly shifting from clinics to homes, where people are being monitored with the help of cameras and other sensors. It is important to resolve the aforementioned prob- lems in order to better assist the aging population, which is the most susceptible (especially during COVID) and has a growing demand to keep them living independently at home.

> æ­¤å¤–ï¼Œéšç§é—®é¢˜é˜»æ­¢äº†è¿™äº›æŠ€æœ¯åœ¨éå…¬å…±åœ°æ–¹çš„ä½¿ç”¨ã€‚ä¾‹å¦‚ï¼Œå¤§å¤šæ•°äººä¸æ„¿æ„åœ¨å®¶ä¸­è¢«æ‘„åƒæœºå½•åˆ¶ï¼Œåœ¨æŸäº›åœ°åŒºï¼ˆå¦‚æµ´å®¤ï¼‰å®‰è£…æ‘„åƒæœºå°†ä¸å¯è¡Œã€‚è¿™äº›é—®é¢˜åœ¨åŒ»ç–—åº”ç”¨ä¸­ç‰¹åˆ«å…³é”®ï¼Œéšç€åŒ»ç–—ç›‘æµ‹ä»è¯Šæ‰€å‘å®¶åº­è½¬ç§»ï¼Œäººä»¬æ­£åœ¨é€šè¿‡æ‘„åƒæœºå’Œå…¶ä»–ä¼ æ„Ÿå™¨ç›‘æµ‹ã€‚ä¸ºäº†æ›´å¥½åœ°å¸®åŠ©è€å¹´äººï¼Œç‰¹åˆ«æ˜¯åœ¨COVIDæœŸé—´ï¼Œæœ€å®¹æ˜“å—åˆ°å½±å“å¹¶éœ€è¦ç‹¬è‡ªç”Ÿæ´»åœ¨å®¶ä¸­çš„è€å¹´äººï¼Œè§£å†³ä¸Šè¿°é—®é¢˜è‡³å…³é‡è¦ã€‚

We believe that WiFi signals can serve as a ubiquitous substitute for RGB images for human sensing in certain instances. Illumination and occlusion have little effect on WiFi-based solutions used for interior monitoring. In addition, they protect individualsâ€™ privacy and the required equipment can be bought at a reasonable price. In fact, most households in developed countries already have WiFi at home, and this technology may be scaled to monitor the well-being of elder people or just identify suspicious behaviors at home.

> æˆ‘ä»¬ç›¸ä¿¡ï¼ŒWiFiä¿¡å·å¯ä»¥åœ¨æŸäº›æƒ…å†µä¸‹ä½œä¸ºRGBå›¾åƒå¯¹äººä½“æ„ŸçŸ¥çš„æ™®éæ›¿ä»£å“ã€‚å…‰ç…§å’Œé®æŒ¡å¯¹ç”¨äºå®¤å†…ç›‘æµ‹çš„åŸºäºWiFiçš„è§£å†³æ–¹æ¡ˆå‡ ä¹æ²¡æœ‰å½±å“ã€‚æ­¤å¤–ï¼Œå®ƒä»¬ä¿æŠ¤ä¸ªäººçš„éšç§ï¼Œæ‰€éœ€çš„è®¾å¤‡ä»·æ ¼åˆç†ã€‚äº‹å®ä¸Šï¼Œå¤§å¤šæ•°å‘è¾¾å›½å®¶çš„å®¶åº­å·²ç»æœ‰äº†WiFiï¼Œè¿™é¡¹æŠ€æœ¯å¯ä»¥æ‰©å±•åˆ°ç›‘æµ‹è€å¹´äººçš„å¥åº·çŠ¶å†µï¼Œæˆ–åœ¨å®¶ä¸­è¯†åˆ«å¯ç–‘è¡Œä¸ºã€‚

The issue we are trying to solve is depicted in Fig. 1 (first row). Given three WiFi transmitters and three aligned receivers, can we detect and recover dense human pose correspondence in clut- tered scenarios with multiple people (Fig. 1 fourth row). It should be noted that many WiFi routers, such as TP-Link AC1750, come with 3 antennas, so our method only requires 2 of these routers. Each of these router is around 30 dollars, which means our entire setup is still way cheaper than LiDAR and radar systems. Many factors make this a difficult task to solve. 

> æˆ‘ä»¬è¯•å›¾è§£å†³çš„é—®é¢˜å¦‚å›¾1ï¼ˆç¬¬1è¡Œï¼‰æ‰€ç¤ºã€‚ç»™å®šä¸‰ä¸ªWiFiå‘å°„å™¨å’Œä¸‰ä¸ªå¯¹é½çš„æ¥æ”¶å™¨ï¼Œæˆ‘ä»¬æ˜¯å¦å¯ä»¥åœ¨å¤šäººçš„æ‚ä¹±æƒ…å†µä¸‹ï¼ˆå›¾1ç¬¬4è¡Œï¼‰æ£€æµ‹å’Œæ¢å¤äººä½“å¯†é›†å§¿æ€å¯¹åº”å…³ç³»ã€‚åº”è¯¥æ³¨æ„åˆ°ï¼Œè®¸å¤šWiFiè·¯ç”±å™¨ï¼ˆä¾‹å¦‚TP-Link AC1750ï¼‰é…å¤‡äº†3ä¸ªå¤©çº¿ï¼Œå› æ­¤æˆ‘ä»¬çš„æ–¹æ³•åªéœ€è¦è¿™äº›è·¯ç”±å™¨ä¸­çš„2ä¸ªã€‚æ¯ä¸ªè·¯ç”±å™¨çš„ä»·æ ¼çº¦ä¸º30ç¾å…ƒï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬æ•´ä¸ªè®¾ç½®ä»ç„¶è¿œè¿œä½äºLiDARå’Œé›·è¾¾ç³»ç»Ÿã€‚è®¸å¤šå› ç´ ä½¿è¿™æˆä¸ºä¸€ä¸ªéš¾ä»¥è§£å†³çš„ä»»åŠ¡ã€‚

First of all, WiFi-based perception[11, 30] is based on the Channel-state-information (CSI) that represents the ratio between the transmitted signal wave and the received signal wave. The CSIs are complex decimal sequences that do not have spatial correspondence to spatial locations, such as the image pixels. 

> é¦–å…ˆï¼ŒåŸºäºWiFiçš„æ„ŸçŸ¥[11,30]æ˜¯åŸºäºä¿¡é“çŠ¶æ€ä¿¡æ¯ï¼ˆCSIï¼‰çš„ï¼Œè¯¥ä¿¡æ¯è¡¨ç¤ºå‘é€ä¿¡å·æ³¢å’Œæ¥æ”¶ä¿¡å·æ³¢çš„æ¯”ç‡ã€‚ CSIæ˜¯å¤æ•°åè¿›åˆ¶åºåˆ—ï¼Œæ²¡æœ‰ä¸ç©ºé—´ä½ç½®çš„å¯¹åº”å…³ç³»ï¼Œä¾‹å¦‚å›¾åƒåƒç´ ã€‚

Secondly, classic techniques rely on accurate measurement of time-of-fly and angle-of-arrival of the signal be- tween the transmitter and receiver [13, 26]. These techniques only locate the objectâ€™s center; moreover, the localization accuracy is only around 0.5 meters due to the random phase shift allowed by the IEEE 802.11n/ac WiFi communication standard and potential interference with electronic devices under similar frequency range such as microwave oven and cellphones.

> å…¶æ¬¡ï¼Œç»å…¸æŠ€æœ¯ä¾èµ–äºä¿¡å·åœ¨å‘å°„æœºå’Œæ¥æ”¶æœºä¹‹é—´çš„æ—¶é—´å’Œè§’åº¦æµ‹é‡[13,26]ã€‚ è¿™äº›æŠ€æœ¯åªèƒ½å®šä½ç‰©ä½“çš„ä¸­å¿ƒï¼›æ­¤å¤–ï¼Œç”±äºIEEE 802.11n/ac WiFié€šä¿¡æ ‡å‡†å…è®¸çš„éšæœºç›¸ä½å˜åŒ–ä»¥åŠä¸å…·æœ‰ç›¸ä¼¼é¢‘ç‡èŒƒå›´çš„ç”µå­è®¾å¤‡ï¼ˆä¾‹å¦‚å¾®æ³¢ç‚‰å’Œæ‰‹æœºï¼‰çš„å¹²æ‰°ï¼Œå®šä½ç²¾åº¦ä»…ä¸º0.5ç±³å·¦å³ã€‚

To address these issues, we derive inspiration from recent pro-posed deep learning architectures in computer vision, and propose a neural network architecture that can perform dense pose estima- tion from WiFi. Fig 1 (bottom row) illustrates how our algorithm is able to estimate dense pose using only WiFi signal in scenarios with occlusion and multiple people.

> ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬ä»è®¡ç®—æœºè§†è§‰ä¸­æœ€è¿‘æå‡ºçš„æ·±åº¦å­¦ä¹ æ¶æ„ä¸­è·å¾—å¯ç¤ºï¼Œå¹¶æå‡ºäº†ä¸€ç§èƒ½å¤Ÿä»WiFiè¿›è¡Œå¯†é›†å§¿æ€ä¼°è®¡çš„ç¥ç»ç½‘ç»œæ¶æ„ã€‚ å›¾1ï¼ˆåº•éƒ¨ï¼‰è¯´æ˜äº†æˆ‘ä»¬çš„ç®—æ³•å¦‚ä½•ä»…ä½¿ç”¨WiFiä¿¡å·åœ¨å­˜åœ¨é®æŒ¡å’Œå¤šäººçš„æƒ…å†µä¸‹ä¼°è®¡å¯†é›†å§¿æ€ã€‚

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/5779ba067584485c9a60db72410e8a3a.png#pic_center)
 *Figure 1: The first row illustrates the hardware setup. The second and third rows are the clips of amplitude and phase of the input WiFi signal. The fourth row contains the dense pose estimation of our algorithm from only the WiFi signal.*

> å›¾1ï¼šç¬¬ä¸€è¡Œè¯´æ˜äº†ç¡¬ä»¶è®¾ç½®ã€‚ç¬¬äºŒè¡Œå’Œç¬¬ä¸‰è¡Œæ˜¯è¾“å…¥WiFiä¿¡å·çš„æŒ¯å¹…å’Œç›¸ä½çš„å‰ªè¾‘ã€‚ç¬¬å››è¡ŒåŒ…å«äº†æˆ‘ä»¬ç®—æ³•ä»…é€šè¿‡WiFiä¿¡å·çš„å¯†é›†å§¿æ€ä¼°è®¡ã€‚

# 2 RELATED WORK

This section briefly describes existing work on dense estimation from images and human sensing from WiFi.

> è¿™ä¸€éƒ¨åˆ†ç®€è¦æè¿°äº†ä»å›¾åƒä¸­è¿›è¡Œå¯†é›†ä¼°è®¡å’Œä»WiFiä¸­è¿›è¡Œäººç±»æ„ŸçŸ¥çš„ç°æœ‰å·¥ä½œã€‚

Our research aims to conduct dense pose estimation via WiFi. In computer vision, the subject of dense pose estimation from pictures and video has received a lot of attention [6, 8, 18, 40]. This task consists of finding the dense correspondence between image pixels and the dense vertices indexes of a 3D human body model. The pioneering work of GÃ¼ler et al. [8] mapped human images to dense correspondences of a human mesh model using deep networks. DensePose is based on instance segmentation architectures such as Mark-RCNN [9], and predicts body-wise UV maps for each pixel, where UV maps are flattened representations of 3d geometry, with coordinate points usually corresponding to the vertices of a 3d dimensional object. In this work, we borrow the same architecture as DensePose [8]; however, our input will not be an image or video, but we use 1D WiFi signals to recover the dense correspondence.

> æˆ‘ä»¬çš„ç ”ç©¶æ—¨åœ¨é€šè¿‡WiFiè¿›è¡Œå¯†é›†å§¿æ€ä¼°è®¡ã€‚åœ¨è®¡ç®—æœºè§†è§‰ä¸­ï¼Œä»å›¾ç‰‡å’Œè§†é¢‘ä¸­è¿›è¡Œå¯†é›†å§¿æ€ä¼°è®¡çš„é—®é¢˜å—åˆ°äº†å¾ˆå¤šå…³æ³¨ [6,8,18,40]ã€‚è¯¥ä»»åŠ¡åŒ…æ‹¬æ‰¾åˆ°å›¾åƒåƒç´ å’Œä¸‰ç»´äººä½“æ¨¡å‹çš„å¯†é›†é¡¶ç‚¹ç´¢å¼•ä¹‹é—´çš„å¯†é›†å¯¹åº”å…³ç³»ã€‚ GÃ¼lerç­‰äººçš„å…ˆé©±å·¥ä½œ[8]ä½¿ç”¨æ·±åº¦ç½‘ç»œå°†äººç±»å›¾åƒæ˜ å°„åˆ°äººä½“ç½‘æ ¼æ¨¡å‹çš„å¯†é›†å¯¹åº”å…³ç³»ã€‚DensePoseåŸºäºå®ä¾‹åˆ†å‰²æ¶æ„ï¼Œå¦‚Mark-RCNN [9]ï¼Œé¢„æµ‹æ¯ä¸ªåƒç´ çš„body-wise UVæ˜ å°„ï¼Œå…¶ä¸­UVæ˜ å°„æ˜¯3då‡ ä½•çš„å‹ç¼©è¡¨ç¤ºï¼Œå…¶åæ ‡ç‚¹é€šå¸¸å¯¹åº”3dç»´å¯¹è±¡çš„é¡¶ç‚¹ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å€Ÿé‰´äº†ä¸DensePose [8]ç›¸åŒçš„æ¶æ„ï¼›ç„¶è€Œï¼Œæˆ‘ä»¬çš„è¾“å…¥ä¸æ˜¯å›¾åƒæˆ–è§†é¢‘ï¼Œè€Œæ˜¯ä½¿ç”¨1D WiFiä¿¡å·æ¥æ¢å¤å¯†é›†å¯¹åº”å…³ç³»ã€‚

Recently, there have been many extensions of DensePose pro- posed, especially in 3D human reconstruction with dense body parts [3, 35, 37, 38]. Shapovalov et al.â€™s [24] work focused on lifting dense pose surface maps to 3D human models without 3D supervision. Their network demonstrates that the dense correspondence alone (without using full 2D RGB images) contains sufficient information to generate posed 3D human body. Compared to previous works on reconstructing 3D humans with sparse 2D keypoints, DensePose annotations are much denser and provide information about the 3D surface instead of 2D body joints.

> æœ€è¿‘ï¼ŒDensePose çš„è®¸å¤šæ‰©å±•å·²ç»è¢«æå‡ºï¼Œç‰¹åˆ«æ˜¯åœ¨å¯†é›†èº«ä½“éƒ¨åˆ†çš„ 3D äººä½“é‡å»ºæ–¹é¢ [3,35,37,38]ã€‚Kapovalov ç­‰äººçš„å·¥ä½œ [24] ç€é‡äºå°†å¯†é›†å§¿æ€è¡¨é¢æ˜ å°„å‡çº§ä¸ºä¸å¸¦ 3D ç›‘ç£çš„ 3D äººä½“æ¨¡å‹ã€‚ä»–ä»¬çš„ç½‘ç»œè¯æ˜ï¼Œä»…å¯†é›†å¯¹åº”ï¼ˆä¸ä½¿ç”¨å®Œæ•´çš„ 2D RGB å›¾åƒï¼‰åŒ…å«ç”Ÿæˆ posed 3D äººä½“æ‰€éœ€çš„å……åˆ†ä¿¡æ¯ã€‚ä¸ä¹‹å‰ç”¨ç¨€ç– 2D å…³é”®ç‚¹é‡å»º 3D äººç±»çš„å·¥ä½œç›¸æ¯”ï¼ŒDensePose æ³¨é‡Šæ›´å¯†é›†ï¼Œæä¾›äº†æœ‰å…³ 3D è¡¨é¢çš„ä¿¡æ¯ï¼Œè€Œä¸æ˜¯ 2D èº«ä½“å…³èŠ‚ã€‚

While there is a extensive literature on detection [19, 20], tracking [4, 34], and dense pose estimation [8, 18] from images and videos, human pose estimation from WiFi or radar is a relatively unexplored problem. At this point, it is important to differentiate the current work on radar-based systems and WiFi. The work of Adib et.al. [2] proposed a Frequency Modulated Continuous Wave (FMCW) radar system (broad bandwidth from 5.56GHz to 7.25GHz) for indoor human localization. A limitation of this system is the specialized hardware for synchronizing the ransmission, refraction, and reflection to compute the Time-of-Flight (ToF). The system reached a resolution of 8.8 cm on body localization. In the following work [1], they improved the system by focusing on a moving per- son and generated a rough single-person outline with depth maps. Recently, they applied deep learning approaches to do fine-grained human pose estimation using a similar system, named RF-Pose [39]. These systems do not work under the IEEE 802.11n/ac WiFi communication standard (40MHz bandwidth centered at 2.4GHz). They rely on additional high-frequency and high-bandwidth electromagnetic fields, which need specialized technology not available to the general public. Recently, significant improvements have been made to radar-based human sensing systems. mmMesh [36] generates 3D human mesh from commercially portable millimeter-wave de- vices. This system can accurately localize the vertices on the human mesh with an average error of 2.47 cm. However, mmMesh does not work well with occlusions since high-frequency radio waves cannot penetrate objects.

> åœ¨å›¾åƒå’Œè§†é¢‘çš„æ£€æµ‹[19,20]ã€è·Ÿè¸ª[4,34]å’Œå¯†é›†å§¿æ€ä¼°è®¡[8,18]æ–¹é¢æœ‰ç€ä¸°å¯Œçš„æ–‡çŒ®ï¼Œä½†ä»WiFiæˆ–é›·è¾¾ä¸­ä¼°è®¡äººä½“å§¿æ€æ˜¯ä¸€ä¸ªç›¸å¯¹æœªè¢«æ¢ç´¢çš„é—®é¢˜ã€‚åœ¨è¿™ä¸€ç‚¹ä¸Šï¼Œé‡è¦çš„æ˜¯è¦åŒºåˆ†å½“å‰åŸºäºé›·è¾¾çš„ç³»ç»Ÿå’ŒWiFiçš„å·¥ä½œã€‚ Adibç­‰äººçš„å·¥ä½œ[2]æå‡ºäº†ä¸€ç§é¢‘ç‡è°ƒåˆ¶è¿ç»­æ³¢ï¼ˆFMCWï¼‰é›·è¾¾ç³»ç»Ÿï¼ˆä»5.56GHzåˆ°7.25GHzçš„å®½å¸¦ï¼‰ç”¨äºå®¤å†…äººä½“å®šä½ã€‚è¯¥ç³»ç»Ÿçš„ä¸€ä¸ªé™åˆ¶æ˜¯ç”¨äºåŒæ­¥ä¼ è¾“ã€æŠ˜å°„å’Œåå°„ä»¥è®¡ç®—æ—¶é—´åˆ°è¾¾ï¼ˆToFï¼‰çš„ä¸“ç”¨ç¡¬ä»¶ã€‚è¯¥ç³»ç»Ÿåœ¨èº«ä½“å®šä½ä¸Šè¾¾åˆ°äº†8.8å˜ç±³çš„åˆ†è¾¨ç‡ã€‚åœ¨éšåçš„å·¥ä½œ[1]ä¸­ï¼Œä»–ä»¬é€šè¿‡å…³æ³¨ç§»åŠ¨çš„äººæ¥æ”¹è¿›è¯¥ç³»ç»Ÿï¼Œå¹¶ç”Ÿæˆäº†ä¸€ä¸ªå¸¦æ·±åº¦å›¾çš„ç²—ç•¥çš„å•äººè½®å»“ã€‚æœ€è¿‘ï¼Œä»–ä»¬åº”ç”¨æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼Œä½¿ç”¨ç±»ä¼¼çš„ç³»ç»Ÿè¿›è¡Œç»†ç²’åº¦äººä½“å§¿æ€ä¼°è®¡ï¼Œç§°ä¸ºRF-Pose[39]ã€‚è¿™äº›ç³»ç»Ÿä¸é€‚ç”¨äºIEEE 802.11n/ac WiFié€šä¿¡æ ‡å‡†ï¼ˆä»¥2.4GHzä¸ºä¸­å¿ƒçš„40MHzå¸¦å®½ï¼‰ã€‚å®ƒä»¬ä¾èµ–äºé¢å¤–çš„é«˜é¢‘å’Œé«˜å¸¦å®½ç”µç£åœºï¼Œéœ€è¦æ™®é€šå…¬ä¼—ä¸å…·å¤‡çš„ä¸“ç”¨æŠ€æœ¯ã€‚æœ€è¿‘ï¼ŒåŸºäºé›·è¾¾çš„äººä½“ä¼ æ„Ÿç³»ç»Ÿå–å¾—äº†é‡å¤§è¿›å±•ã€‚

Unlike the above radar systems, the WiFi-based solution [11, 30] used off-the-shelf WiFi adapters and 3dB omnidirectional antennas. The signal propagate as the IEEE 802.11n/ac WiFi data packages transmitting between antennas, which does not introduce addi- tional interference. However, WiFi-based person localization using the traditional time-of-flight (ToF) method is limited by its wave- length and signal-to-noise ratio. Most existing approaches only conduct center mass localization [5, 27] and single-person action classification [25, 29]. Recently, Fei Wang et.al. [31] demonstrated that it is possible to detect 17 2D body joints and perform 2D se- mantic body segmentation mask using only WiFi signals. In this work, we go beyond [31] by estimating dense body pose, with much more accuracy than the 0.5m that the WiFi signal can pro- vide theoretically. Our dense posture outputs push above WiFiâ€™s signal constraint in body localization, paving the road for complete dense 2D and possibly 3D human body perception through WiFi. To achieve this, instead of directly training a randomly initialized WiFi-based model, we explored rich supervision information to improve both the performance and training efficiency, such as uti- lizing the CSI phase, adding keypoint detection branch, and transfer learning from the image-based model.

> ä¸ä¸Šè¿°çš„é›·è¾¾ç³»ç»Ÿä¸åŒï¼ŒåŸºäºWiFiçš„è§£å†³æ–¹æ¡ˆ[11,30]ä½¿ç”¨äº†ç°æˆçš„WiFié€‚é…å™¨å’Œ3dBå…¨å‘å¤©çº¿ã€‚ä¿¡å·ä¼ æ’­æ˜¯IEEE 802.11n/ac WiFiæ•°æ®åŒ…åœ¨å¤©çº¿ä¹‹é—´ä¼ è¾“ï¼Œä¸ä¼šå¼•å…¥é¢å¤–çš„å¹²æ‰°ã€‚ç„¶è€Œï¼Œä½¿ç”¨ä¼ ç»Ÿçš„æ—¶é—´-é£è¡Œï¼ˆToFï¼‰æ–¹æ³•çš„WiFiåŸºäºäººå‘˜å®šä½å—å…¶æ³¢é•¿å’Œä¿¡å™ªæ¯”çš„é™åˆ¶ã€‚å¤§å¤šæ•°ç°æœ‰çš„æ–¹æ³•ä»…è¿›è¡Œä¸­å¿ƒè´¨é‡å®šä½[5,27]å’Œå•äººè¡ŒåŠ¨åˆ†ç±»[25,29]ã€‚æœ€è¿‘ï¼ŒFei Wangç­‰äºº[31]è¯æ˜äº†å¯ä»¥ä½¿ç”¨ä»…WiFiä¿¡å·æ£€æµ‹17ä¸ª2Dèº«ä½“å…³èŠ‚å¹¶æ‰§è¡Œ2Dè¯­ä¹‰èº«ä½“åˆ†å‰²æ©ç ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è¶…è¶Šäº†[31]ï¼Œé€šè¿‡ä¼°è®¡å¯†é›†çš„èº«ä½“å§¿æ€ï¼Œæ¯”WiFiä¿¡å·å¯ä»¥æä¾›çš„ç†è®ºä¸Šçš„0.5ç±³æ›´ç²¾ç¡®ã€‚æˆ‘ä»¬å¯†é›†çš„å§¿åŠ¿è¾“å‡ºè¶…å‡ºäº†WiFiåœ¨èº«ä½“å®šä½ä¸­çš„ä¿¡å·é™åˆ¶ï¼Œä¸ºå®Œæ•´çš„å¯†é›†2Då’Œå¯èƒ½çš„3Däººä½“æ„ŸçŸ¥é€šè¿‡WiFié“ºå¹³äº†é“è·¯ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬ä¸æ˜¯ç›´æ¥è®­ç»ƒéšæœºåˆå§‹åŒ–çš„åŸºäºWiFiçš„æ¨¡å‹ï¼Œè€Œæ˜¯æ¢ç´¢ä¸°å¯Œçš„ç›‘ç£ä¿¡æ¯æ¥æé«˜æ€§èƒ½å’Œè®­ç»ƒæ•ˆç‡ï¼Œä¾‹å¦‚åˆ©ç”¨CSIç›¸ä½ï¼Œæ·»åŠ å…³é”®ç‚¹æ£€æµ‹åˆ†æ”¯ï¼Œå¹¶ä»åŸºäºå›¾åƒçš„æ¨¡å‹è¿›è¡Œè½¬ç§»å­¦ä¹ ã€‚

# 3 METHODS

Our approach produces UV coordinates of the human body surface from WiFi signals using three components: first, the raw CSI signals are cleaned by amplitude and phase sanitization. Then, a two-branch encoder-decoder network performs domain translation from sanitized CSI samples to 2D feature maps that resemble images. The 2D features are then fed to a modified DensePose-RCNN architecture [8] to estimate the UV map, a representation of the dense correspondence between 2D and 3D humans. To improve the training of our WiFi-input network, we conduct transfer learning, where we minimize the differences between the multi-level fea- ture maps produced by images and those produced by WiFi signals before training our main network.

> æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ä¸‰ä¸ªéƒ¨åˆ†ä»WiFiä¿¡å·ç”Ÿæˆäººä½“çš„UVåæ ‡ï¼šé¦–å…ˆï¼ŒåŸå§‹CSIä¿¡å·é€šè¿‡æŒ¯å¹…å’Œç›¸ä½æ¸…æ´å¤„ç†ï¼›ç„¶åï¼Œä¸€ä¸ªåŒåˆ†æ”¯ç¼–ç å™¨-è§£ç å™¨ç½‘ç»œä»æ¸…æ´çš„CSIæ ·æœ¬è¿›è¡ŒåŸŸåè½¬æ¢ï¼Œå¾—åˆ°ç±»ä¼¼å›¾åƒçš„2Dç‰¹å¾å›¾ï¼›æœ€åï¼Œ2Dç‰¹å¾å›¾è¾“å…¥åˆ°ä¿®æ”¹è¿‡çš„DensePose-RCNNä½“ç³»ç»“æ„[8]ä¸­ï¼Œä¼°è®¡UVå›¾ï¼Œè¡¨ç¤º2Då’Œ3Däººä½“ä¹‹é—´å¯†é›†å¯¹åº”å…³ç³»çš„è¡¨ç¤ºã€‚ä¸ºäº†æé«˜æˆ‘ä»¬WiFiè¾“å…¥ç½‘ç»œçš„è®­ç»ƒï¼Œæˆ‘ä»¬è¿›è¡Œè¿ç§»å­¦ä¹ ï¼Œé€šè¿‡æœ€å°åŒ–å›¾åƒå’ŒWiFiä¿¡å·ç”Ÿæˆçš„å¤šå±‚ç‰¹å¾å›¾çš„å·®å¼‚ï¼Œåœ¨è®­ç»ƒä¸»ç½‘ç»œä¹‹å‰æé«˜ç‰¹å¾å›¾çš„ç›¸ä¼¼åº¦ã€‚

The raw CSI data are sampled in 100Hz as complex values over 30 subcarrier frequencies (linearly spaced within $2.4GHz \pm 20MHz$) transmitting among 3 emitter antennas and 3 reception antennas (see Figure 2). Each CSI sample contains a $3 \times 3$ real integer matrix and a $3 \times 3$ imaginary integer matrix. The inputs of our network contained 5 consecutive CSI samples under 30 frequencies, which are organized in a $150 \times 3 \times 3$ amplitude tensor and a $150 \times 3 \times 3$ phase tensor respectively. Our network outputs include a $17 \times 56 \times 56$ tensor of keypoint heatmaps (one $56 \times 56$ map for each of the 17 kepoints) and a $25 \times 112 \times 112$ tensor of UV maps (one $112 \times 112$ map for each of the 24 body parts with one additional map for background).

> æˆ‘ä»¬çš„åŸå§‹ CSI æ•°æ®æ˜¯åœ¨ 100Hz çš„é¢‘ç‡ä»¥å¤æ•°å€¼å½¢å¼å¯¹ 30 ä¸ªå­è½½æ³¢é¢‘ç‡ï¼ˆåœ¨ $2.4GHz \pm 20MHz$ èŒƒå›´å†…çº¿æ€§åˆ†å¸ƒï¼‰é‡‡æ ·çš„ã€‚å…¶ä¸­ï¼Œ3 ä¸ªå‘å°„å¤©çº¿å’Œ 3 ä¸ªæ¥æ”¶å¤©çº¿ä¹‹é—´çš„æ•°æ®ä¼ è¾“ï¼ˆè§å›¾ 2ï¼‰ã€‚æ¯ä¸ª CSI æ ·æœ¬åŒ…å«ä¸€ä¸ª $3 \times 3$ çš„å®æ•´æ•°çŸ©é˜µå’Œä¸€ä¸ª $3 \times 3$ çš„è™šæ•´æ•°çŸ©é˜µã€‚æˆ‘ä»¬çš„ç½‘ç»œçš„è¾“å…¥åŒ…å« 5 ä¸ªè¿ç»­çš„ CSI æ ·æœ¬ä»¥åŠ 30 ä¸ªé¢‘ç‡ï¼Œå®ƒä»¬ä»¥ $150 \times 3 \times 3$ çš„å¹…å€¼å¼ é‡å’Œ $150 \times 3 \times 3$ çš„ç›¸ä½å¼ é‡çš„å½¢å¼ç»„ç»‡ã€‚æˆ‘ä»¬çš„ç½‘ç»œè¾“å‡ºåŒ…æ‹¬ä¸€ä¸ª $17 \times 56 \times 56$ çš„å…³é”®ç‚¹çƒ­å›¾å¼ é‡ï¼ˆæ¯ä¸ª 17 ä¸ªå…³é”®ç‚¹æœ‰ä¸€ä¸ª $56 \times 56$ çš„åœ°å›¾ï¼‰å’Œä¸€ä¸ª $25 \times 112 \times 112$ çš„ UV åœ°å›¾å¼ é‡ï¼ˆæ¯ä¸ª 24 ä¸ªäººä½“éƒ¨ä½æœ‰ä¸€ä¸ª $112 \times 112$ çš„åœ°å›¾ï¼Œå¦å¤–è¿˜æœ‰ä¸€ä¸ªèƒŒæ™¯å›¾ï¼‰ã€‚

## 3.1 Phase Sanitization

The raw CSI samples are noisy with random phase drift and flip (see Figure 3(b)). Most WiFi-based solutions disregard the phase of CSI signals and rely only on their amplitude (see Figure 3 (a)). As shown in our experimental validation, discarding the phase information have a negative impact on the performance of our model. In this section, we perform sanitization to obtain stable phase values to enable full use of the CSI information.

> åŸå§‹çš„ CSI æ ·æœ¬å…·æœ‰éšæœºç›¸ä½æ¼‚ç§»å’Œç¿»è½¬çš„å™ªéŸ³ï¼ˆè§å›¾ 3(b)ï¼‰ã€‚å¤§å¤šæ•°åŸºäº WiFi çš„è§£å†³æ–¹æ¡ˆå¿½ç•¥ CSI ä¿¡å·çš„ç›¸ä½ï¼Œä»…ä¾èµ–å®ƒä»¬çš„æŒ¯å¹…ï¼ˆè§å›¾ 3(a)ï¼‰ã€‚å¦‚æˆ‘ä»¬çš„å®éªŒéªŒè¯æ‰€ç¤ºï¼Œä¸¢å¼ƒç›¸ä½ä¿¡æ¯ä¼šå¯¹æˆ‘ä»¬æ¨¡å‹çš„æ€§èƒ½äº§ç”Ÿè´Ÿé¢å½±å“ã€‚åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬æ‰§è¡Œæ¸…æ´ä»¥è·å¾—ç¨³å®šçš„ç›¸ä½å€¼ï¼Œä»¥å®ç°å¯¹ CSI ä¿¡æ¯çš„å……åˆ†åˆ©ç”¨ã€‚

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/be2eaefd972c4568a154f7923a42ea5e.png#pic_center)

*Figure 2: CSI samples from Wifi. (a) the layout of WiFi devices and human bodies, and (b) the $3 \times 3$ tensor dimension corresponds to the $3 \times 3$ transmitter-receiver antenna pairs. For instance, $E1$ denotes the first emitter and $R1$ denotes the first receiver, etc. By incorporating the 5 consecutive complex-valued CSI samples (100 samples/second) under 30 subcarrier frequencies, the two input tensors to our network are a $150 \times 3 \times 3$ amplitude tensor and a $150 \times 3 \times 3$ phase tensor.*

> å›¾2ï¼šWiFiçš„CSIæ ·æœ¬ã€‚ï¼ˆaï¼‰WiFiè®¾å¤‡å’Œäººä½“çš„å¸ƒå±€ï¼Œï¼ˆbï¼‰$3 \times 3$ å¼ é‡ç»´åº¦å¯¹åº”äº$3 \times 3$ å‘å°„æœº-æ¥æ”¶æœºå¤©çº¿å¯¹ã€‚ä¾‹å¦‚ï¼Œ$E1$è¡¨ç¤ºç¬¬ä¸€ä¸ªå‘å°„æœºï¼Œ$R1$è¡¨ç¤ºç¬¬ä¸€ä¸ªæ¥æ”¶æœºï¼Œç­‰ã€‚é€šè¿‡ç»“åˆ30ä¸ªå­è½½æ³¢é¢‘ç‡ä¸‹çš„5ä¸ªè¿ç»­å¤æ‚å€¼CSIæ ·æœ¬ï¼ˆæ¯ç§’100ä¸ªæ ·æœ¬ï¼‰ï¼Œæˆ‘ä»¬ç½‘ç»œçš„ä¸¤ä¸ªè¾“å…¥å¼ é‡æ˜¯ $150 \times 3 \times 3$ æŒ¯å¹…å¼ é‡å’Œ $150 \times 3 \times 3$ ç›¸ä½å¼ é‡ã€‚

In raw CSI samples (5 consecutive samples visualized in Figure 3(a-b)), the amplitude (A) and phase ($\Phi$) of each complex element $z = a +bi$ are computed using the formulation $ğ´ = \sqrt{a^2 + b^2}$ and $\Phi = \arctan(b/a)$. Note that the range of the arctan function is from $âˆ’\pi$ to $\pi$ and the phase values outside this range get wrapped, leading to a discontinuity in phase values. Our first sanitization step is to unwrap the phase following [10]:

> åœ¨åŸå§‹çš„CSIæ ·æœ¬ï¼ˆåœ¨å›¾3ï¼ˆa-bï¼‰ä¸­å¯è§†åŒ–çš„5ä¸ªè¿ç»­æ ·æœ¬ï¼‰ä¸­ï¼Œæ¯ä¸ªå¤æ•°å…ƒç´ $z=a+bi$çš„æŒ¯å¹…ï¼ˆAï¼‰å’Œç›¸ä½ï¼ˆ$\Phi$ï¼‰è®¡ç®—ä½¿ç”¨ä»¥ä¸‹å…¬å¼ï¼š$A=\sqrt{a^2 + b^2}$ å’Œ $\Phi=\arctan(b/a)$ã€‚è¯·æ³¨æ„ï¼Œarctanå‡½æ•°çš„èŒƒå›´ä»$-\pi$åˆ°$\pi$ï¼Œè¶…å‡ºæ­¤èŒƒå›´çš„ç›¸ä½å€¼è¢«åŒ…è£…ï¼Œå¯¼è‡´ç›¸ä½å€¼ä¸è¿ç»­ã€‚æˆ‘ä»¬çš„ç¬¬ä¸€æ­¥æ¸…ç†æ­¥éª¤æ˜¯æ ¹æ®[10]æ‹†å¼€ç›¸ä½ï¼š

**Eq. 1**
$$
\begin{matrix}
\Delta \phi_{i,j} = \Phi_{i, j+1} - \Phi_{i,j} \\
if \Delta \phi_{i, j} > \pi, \Phi_{i, j+1} = \Phi_{i, j} + \Delta \phi_{i, j} - 2 \pi \\
if \Delta \phi_{i, j} < -\pi, \Phi_{i, j+1} = \Phi_{i, j} + \Delta \phi_{i, j} + 2 \pi
\end{matrix}
$$

where $i$ denotes the index of the measurements in the five consecutive samples, and $j$ denotes the index of the subcarriers(frequencies). Following unwrapping, each of the flipping phase curves in Fig- ure 3(b) are restored to continuous curves in Figure 3(c).

> å…¶ä¸­ï¼Œ$i$ è¡¨ç¤ºæµ‹é‡åœ¨è¿ç»­äº”ä¸ªæ ·æœ¬ä¸­çš„ç´¢å¼•ï¼Œ$j$ è¡¨ç¤ºå­è½½æ³¢ï¼ˆé¢‘ç‡ï¼‰çš„ç´¢å¼•ã€‚åœ¨è§£é™¤ç¿»è½¬ç›¸ä½æ›²çº¿ä¹‹åï¼Œå›¾ 3(b) ä¸­çš„æ›²çº¿å°†åœ¨å›¾ 3(c) ä¸­è¢«æ¢å¤ä¸ºè¿ç»­æ›²çº¿ã€‚

Observe that among the 5 phase curves captured in 5 consecutive samples in Figure 3(c), there are random jiterings that break the temporal order among the samples. To keep the temporal order of signals, previous work [23] mentioned linear fitting as a popular approach. However, directly applying linear fitting to Figure 3(c) further amplified the jitering instead of fixing it (see the failed results in Figure 3(d)).

> åœ¨å›¾3(c)ä¸­ï¼Œå¯ä»¥çœ‹åˆ°5ä¸ªæ ·æœ¬ä¸­çš„5ä¸ªç›¸ä½æ›²çº¿ï¼Œå­˜åœ¨éšæœºæŠ–åŠ¨ï¼Œæ‰“ç ´äº†æ ·æœ¬ä¹‹é—´çš„æ—¶é—´é¡ºåºã€‚æ ¹æ®ä»¥å‰çš„ç ”ç©¶[23]ï¼Œä¿æŒä¿¡å·çš„æ—¶é—´é¡ºåºæ˜¯é€šè¿‡çº¿æ€§æ‹Ÿåˆçš„ä¸€ç§å¸¸ç”¨æ–¹æ³•ã€‚ä½†æ˜¯ï¼Œç›´æ¥å¯¹å›¾3(c)è¿›è¡Œçº¿æ€§æ‹Ÿåˆå¹¶ä¸èƒ½è§£å†³é—®é¢˜ï¼Œåè€Œä¼šåŠ å‰§æŠ–åŠ¨ï¼ˆè¯·å‚è§å›¾3(d)çš„å¤±è´¥ç»“æœï¼‰ã€‚

From Figure 3(c), we use median and uniform filters to eliminate outliers in both the time and frequency domain which leads to Figure 3(e). Finally, we obtain the fully sanitized phase values by applying the linear fitting method following the equations below:

> æˆ‘ä»¬åœ¨å›¾3(c)ä¸­ä½¿ç”¨ä¸­ä½æ•°å’Œå‡åŒ€æ»¤æ³¢å™¨ï¼Œåœ¨æ—¶é—´å’Œé¢‘ç‡åŸŸä¸­æ¶ˆé™¤å¼‚å¸¸å€¼ï¼Œå¾—åˆ°å›¾3(e)ã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡åº”ç”¨ä»¥ä¸‹æ–¹ç¨‹æ¥è·å¾—å®Œå…¨ç»è¿‡æ¶ˆæ¯’çš„ç›¸ä½å€¼ï¼š

**Eq. 2**

$$
\begin{matrix}
a_1 = \frac{\Phi_{F} - \Phi_{1}}{2 \pi F} \\
a_0 = \frac{1}{F} \sum_{1 \leq f \leq F} \phi_f \\
\hat{\phi_f} = \phi_f - (a_1 f + a_0)
\end{matrix}
$$

where $F$ denotes the largest subcarrier index (30 in our case) and $\hat{\phi_f}$ is the sanitized phase values at subcarrier $f$ (the f th frequency).

> è¿™é‡Œï¼Œ$F$è¡¨ç¤ºå­è½½æ³¢ç´¢å¼•çš„æœ€å¤§å€¼ï¼ˆåœ¨æˆ‘ä»¬çš„æƒ…å†µä¸‹ä¸º30ï¼‰ï¼Œ$\hat{\phi_f}$æ˜¯å­è½½æ³¢$f$ï¼ˆç¬¬$f$ä¸ªé¢‘ç‡ï¼‰ä¸Šçš„ç»è¿‡çº¯åŒ–çš„ç›¸ä½å€¼ã€‚

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/10a8fdea5a8d4759963e430912deb627.png#pic_center)
*Figure 3: Sanitization steps of CSI sequences described in Section 3.1. In each subfigure, we plot five consecutive samples (five colored curves) each containing CSI data of 30 IEEE 802.11n/ac sub-Carrier frequencies (horizontal axis).*

> å›¾3ï¼šåœ¨3.1èŠ‚ä¸­æè¿°çš„CSIåºåˆ—çš„æ¶ˆæ¯’æ­¥éª¤ã€‚åœ¨æ¯ä¸ªå­å›¾ä¸­ï¼Œæˆ‘ä»¬ç»˜åˆ¶äº†äº”ä¸ªè¿ç»­çš„æ ·æœ¬ï¼ˆäº”æ¡å½©è‰²æ›²çº¿ï¼‰ï¼Œæ¯ä¸ªæ ·æœ¬åŒ…å«30ä¸ªIEEE 802.11n/acå­è½½æ³¢é¢‘ç‡çš„CSIæ•°æ®ï¼ˆæ°´å¹³è½´ï¼‰ã€‚

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/a23101b608fa4019813d93e6ab1f79d9.png#pic_center)
*Figure 4: Modality Translation Network. Two encoders extract the features from the amplitude and phase in the CSI domain. Then the features are fused and reshaped before going through an encoder-decoder network. The output is a 3 Ã— 720 Ã— 1280 feature map in the image domain.*

> å›¾4ï¼šæ¨¡æ€è½¬æ¢ç½‘ç»œã€‚ä¸¤ä¸ªç¼–ç å™¨ä»CSIåŸŸçš„æŒ¯å¹…å’Œç›¸ä½ä¸­æå–ç‰¹å¾ã€‚ç„¶åèåˆå’Œé‡å¡‘è¿™äº›ç‰¹å¾ï¼Œå†é€šè¿‡ç¼–ç å™¨-è§£ç å™¨ç½‘ç»œå¤„ç†ã€‚è¾“å‡ºæ˜¯å›¾åƒåŸŸä¸­3Ã—720Ã—1280çš„ç‰¹å¾å›¾ã€‚

## 3.2 Modality Translation Network

In order to estimate the UV maps in the spatial domain from the 1D CSI signals, we first transform the network inputs from the CSI domain to the spatial domain. This is done with the Modality Translation Network (see Figure 4). We first extract the CSI latent space features using two encoders, one for the amplitude tensor and the other for the phase tensor, where both tensors have the size of $150 \times 3 \times 3$ (5 consecutive samples, 30 frequencies, 3 emitters and 3 receivers). Previous work on human sensing with WiFi [30] stated that Convolutional Neural Network (CNN) can be used to extract spatial features from the last two dimensions (the $3 \times 3$ transmitting sensor pairs) of the input tensors. 

> ä¸ºäº†ä»1D CSIä¿¡å·ä¸­ä¼°è®¡å‡ºç©ºé—´åŸŸä¸­çš„UVæ˜ å°„ï¼Œæˆ‘ä»¬é¦–å…ˆå°†ç½‘ç»œè¾“å…¥ä»CSIåŸŸè½¬æ¢åˆ°ç©ºé—´åŸŸã€‚è¿™æ˜¯é€šè¿‡æ¨¡æ€è½¬æ¢ç½‘ç»œï¼ˆè§å›¾4ï¼‰å®Œæˆçš„ã€‚æˆ‘ä»¬é¦–å…ˆä½¿ç”¨ä¸¤ä¸ªç¼–ç å™¨æå–CSIæ½œåœ¨ç©ºé—´ç‰¹å¾ï¼Œä¸€ä¸ªç”¨äºæŒ¯å¹…å¼ é‡ï¼Œå¦ä¸€ä¸ªç”¨äºç›¸ä½å¼ é‡ï¼Œå…¶ä¸­ä¸¤ä¸ªå¼ é‡çš„å¤§å°ä¸º150 Ã— 3 Ã— 3ï¼ˆ5ä¸ªè¿ç»­æ ·æœ¬ï¼Œ30ä¸ªé¢‘ç‡ï¼Œ3ä¸ªå‘å°„å™¨å’Œ3ä¸ªæ¥æ”¶å™¨ï¼‰ã€‚å…³äºWiFiçš„äººç±»æ„ŸçŸ¥çš„å…ˆå‰ç ”ç©¶[30]æŒ‡å‡ºï¼Œå¯ä»¥ä½¿ç”¨å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ä»è¾“å…¥å¼ é‡çš„æœ€åä¸¤ä¸ªç»´åº¦ï¼ˆ3 Ã— 3ä¼ è¾“ä¼ æ„Ÿå™¨å¯¹ï¼‰ä¸­æå–ç©ºé—´ç‰¹å¾ã€‚

We, on the other hand, believe that locations in the $3 \times 3$ feature map do not correlate with the locations in the 2D scene. More specifically, as depicted in Figure 2(b), the element that is colored in blue represents a 1D summary of the entire scene captured by emitter 1 and receiver 3 (E1 - R3), instead of local spatial information of the top right corner of the 2D scene. Therefore, we consider that each of the 1350 elements (in both tensors) captures a unique 1D summary of the entire scene. Following this idea, the amplitude and phase tensors are flattened and feed into two separate multi-layer perceptrons (MLP) to obtain their features in the CSI latent space. We concatenated the 1D features from both encoding branches, then the combined tensor is fed to another MLP to perform feature fusion.

> æˆ‘ä»¬ç›¸ä¿¡ï¼Œ$3 \times 3$ ç‰¹å¾æ˜ å°„ä¸­çš„ä½ç½®ä¸äºŒç»´åœºæ™¯ä¸­çš„ä½ç½®ä¸ç›¸å…³ã€‚æ›´å…·ä½“åœ°ï¼Œå¦‚å›¾ 2(b) æ‰€ç¤ºï¼Œè“è‰²å…ƒç´ ä»£è¡¨ç”±å‘å°„å™¨ 1 å’Œæ¥æ”¶å™¨ 3 (E1 - R3) æ•æ‰çš„æ•´ä¸ªåœºæ™¯çš„ 1D æ‘˜è¦ï¼Œè€Œä¸æ˜¯äºŒç»´åœºæ™¯å³ä¸Šè§’çš„å±€éƒ¨ç©ºé—´ä¿¡æ¯ã€‚å› æ­¤ï¼Œæˆ‘ä»¬è®¤ä¸ºä¸¤ä¸ªå¼ é‡ä¸­çš„æ¯ä¸ª 1350 ä¸ªå…ƒç´ éƒ½æ•æ‰äº†æ•´ä¸ªåœºæ™¯çš„ç‹¬ç‰¹ 1D æ‘˜è¦ã€‚éµå¾ªè¿™ä¸ªæ€æƒ³ï¼ŒæŒ¯å¹…å’Œç›¸ä½å¼ é‡è¢«å±•å¹³ï¼Œå¹¶è¾“å…¥åˆ°ä¸¤ä¸ªç‹¬ç«‹çš„å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰ä¸­ï¼Œä»¥è·å¾—å…¶åœ¨ CSI æ½œåœ¨ç©ºé—´ä¸­çš„ç‰¹å¾ã€‚æˆ‘ä»¬å°†ä¸¤ä¸ªç¼–ç åˆ†æ”¯çš„ 1D ç‰¹å¾è¿æ¥èµ·æ¥ï¼Œå†å°†ç»„åˆçš„å¼ é‡è¾“å…¥åˆ°å¦ä¸€ä¸ª MLP ä¸­ï¼Œä»¥æ‰§è¡Œç‰¹å¾èåˆã€‚

The next step is to transform the CSI latent space features to feature maps in the spatial domain. As shown in Figure 4, the fused 1D feature is reshaped into a 24 Ã— 24 2D feature map. Then, we extract the spatial information by applying two convolution blocks and obtain a more condensed map with the spatial dimension of 6Ã—6. Finally, four deconvolution layers are used to upsample the encoded feature map in low dimensions to the size of 3 Ã— 720 Ã— 1280. We set such an output tensor size to match the dimension commonly used in RGB-image-input network. We now have a scene representation in the image domain generated by WiFi signals.

> ä¸‹ä¸€æ­¥æ˜¯å°† CSI æ½œåœ¨ç©ºé—´ç‰¹å¾è½¬æ¢ä¸ºç©ºé—´åŸŸå†…çš„ç‰¹å¾å›¾ã€‚å¦‚å›¾4æ‰€ç¤ºï¼Œèåˆçš„1Dç‰¹å¾è¢«é‡å¡‘æˆä¸€ä¸ª24Ã—24çš„2Dç‰¹å¾å›¾ã€‚ç„¶åï¼Œæˆ‘ä»¬é€šè¿‡åº”ç”¨ä¸¤ä¸ªå·ç§¯å—æ¥æå–ç©ºé—´ä¿¡æ¯ï¼Œå¹¶è·å¾—å…·æœ‰ç©ºé—´ç»´åº¦ä¸º6Ã—6çš„æ›´ç´§å‡‘çš„å›¾ã€‚æœ€åï¼Œå››ä¸ªåå·ç§¯å±‚è¢«ç”¨æ¥å°†ç¼–ç çš„ä½ç»´ç‰¹å¾å›¾ä¸Šé‡‡æ ·åˆ°3Ã—720Ã—1280çš„å¤§å°ã€‚æˆ‘ä»¬å°†è¿™æ ·çš„è¾“å‡ºå¼ é‡å¤§å°è®¾ç½®ä¸ºä¸å¸¸ç”¨çš„RGBå›¾åƒè¾“å…¥ç½‘ç»œçš„ç»´åº¦ç›¸åŒ¹é…ã€‚æˆ‘ä»¬ç°åœ¨å·²ç»æœ‰äº†ä¸€ä¸ªé€šè¿‡WiFiä¿¡å·ç”Ÿæˆçš„å›¾åƒåŸŸå†…çš„åœºæ™¯è¡¨ç¤ºã€‚

## 3.3 WiFi-DensePose RCNN

After we obtain the $3 \times 720 \times 1280$ scene representation in the image domain, we can utilize image-based methods to predict the UV maps of human bodies. State-of-the-art pose estimation algorithms are two-stage; first, they run an independent person detector to estimate the bounding box and then conduct pose estimation from person-wise image patches. However, as stated before, each element in our CSI input tensors is a summary of the entire scene. It is not possible to extract the signals corresponding to a single person from a group of people in the scene. Therefore, we decide to adopt a network structure similar to DensePose-RCNN [8], since it can predict the dense correspondence of multiple humans in an end-to-end fashion.

> åœ¨æˆ‘ä»¬è·å¾—å›¾åƒåŸŸä¸­å¤§å°ä¸º $3 \times 720 \times 1280$ çš„åœºæ™¯è¡¨ç¤ºåï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨åŸºäºå›¾åƒçš„æ–¹æ³•é¢„æµ‹äººä½“çš„ UV å›¾ã€‚æœ€å…ˆè¿›çš„å§¿æ€ä¼°è®¡ç®—æ³•æ˜¯åˆ†ä¸¤æ­¥è¿›è¡Œçš„ï¼šé¦–å…ˆï¼Œå®ƒä»¬è¿è¡Œç‹¬ç«‹çš„äººç‰©æ£€æµ‹å™¨æ¥ä¼°è®¡è¾¹ç•Œæ¡†ï¼Œç„¶åå¯¹ä»¥äººä¸ºåŸºå‡†çš„å›¾åƒå—è¿›è¡Œå§¿æ€ä¼°è®¡ã€‚ç„¶è€Œï¼Œæ­£å¦‚å‰é¢æ‰€è¯´ï¼Œæˆ‘ä»¬ CSI è¾“å…¥å¼ é‡ä¸­çš„æ¯ä¸ªå…ƒç´ éƒ½æ˜¯æ•´ä¸ªåœºæ™¯çš„æ¦‚æ‹¬ã€‚æ— æ³•ä»åœºæ™¯ä¸­çš„ä¸€ç¾¤äººä¸­æå–å•ä¸ªäººçš„ä¿¡å·ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å†³å®šé‡‡ç”¨ç±»ä¼¼äº DensePose-RCNN [8] çš„ç½‘ç»œç»“æ„ï¼Œå› ä¸ºå®ƒå¯ä»¥ä»¥ç«¯åˆ°ç«¯çš„æ–¹å¼é¢„æµ‹å¤šä¸ªäººçš„å¯†é›†å¯¹åº”å…³ç³»ã€‚

More specifically, in the WiFi-DensePose RCNN (Figure 5), we extract the spatial features from the obtained $3 \times 720 \times 1280$ image-like feature map using the ResNet-FPN backbone [14]. Then, the output will go through the region proposal network [20]. To bet- ter exploit the complementary information of different sources, the next part of our network contains two branches: DensePose head and Keypoint head. Estimating keypoint locations is more reliable than estimating dense correspondences, so we can train our network to use keypoints to restrict DensePose predictions from getting too far from the body joints of humans. 

> å…·ä½“æ¥è¯´ï¼Œåœ¨WiFi-DensePose RCNNï¼ˆå›¾5ï¼‰ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ResNet-FPNéª¨å¹²[14]ä»è·å¾—çš„$3 \times 720 \times 1280$å›¾åƒç±»ç‰¹å¾å›¾ä¸­æå–ç©ºé—´ç‰¹å¾ã€‚ç„¶åï¼Œè¾“å‡ºå°†é€šè¿‡åŒºåŸŸæè®®ç½‘ç»œ[20]ã€‚ä¸ºäº†æ›´å¥½åœ°åˆ©ç”¨ä¸åŒæ¥æºçš„è¡¥å……ä¿¡æ¯ï¼Œæˆ‘ä»¬çš„ç½‘ç»œçš„ä¸‹ä¸€éƒ¨åˆ†åŒ…å«ä¸¤ä¸ªåˆ†æ”¯ï¼šDensePoseå¤´å’Œå…³é”®ç‚¹å¤´ã€‚ä¼°è®¡å…³é”®ç‚¹ä½ç½®æ¯”ä¼°è®¡å¯†é›†å¯¹åº”æ›´å¯é ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥è®­ç»ƒæˆ‘ä»¬çš„ç½‘ç»œï¼Œä½¿å…¶ä½¿ç”¨å…³é”®ç‚¹é™åˆ¶DensePoseé¢„æµ‹ï¼Œä½¿å…¶ä¸ä¼šè¿œç¦»äººç±»èº«ä½“å…³èŠ‚ã€‚

The DensePose head utilizes a Fully Convolutional Network (FCN) [16] to densely predict human part labels and surface coordinates (UV coordinates) within each part, while the keypoint head uses FCN to estimate the keypoint heatmap. The results are combined and then fed into the refinement unit of each branch, where each refinement unit consists of two convolutional blocks followed by an FCN. The network outputs a $17 \times 56 \times 56$ keypoint mask and a $25 \times 112 \times 112$ IUV map. The process is demonstrated in Figure 5. It should be noted that the modality translation network and the WiFi-DensePose RCNN are trained together.

> DensePose head ä½¿ç”¨å®Œå…¨å·ç§¯ç½‘ç»œï¼ˆFCNï¼‰ [16] å¯†é›†åœ°é¢„æµ‹æ¯ä¸ªéƒ¨åˆ†å†…çš„äººä½“éƒ¨åˆ†æ ‡ç­¾å’Œè¡¨é¢åæ ‡ï¼ˆUV åæ ‡ï¼‰ï¼Œè€Œ Keypoint head ä½¿ç”¨ FCN ä¼°è®¡å…³é”®ç‚¹çƒ­å›¾ã€‚ç»“æœç»“åˆåœ¨ä¸€èµ·ï¼Œç„¶åè¾“å…¥åˆ°æ¯ä¸ªåˆ†æ”¯çš„ç»†åŒ–å•å…ƒä¸­ï¼Œå…¶ä¸­æ¯ä¸ªç»†åŒ–å•å…ƒç”±ä¸¤ä¸ªå·ç§¯å—å’Œä¸€ä¸ª FCN ç»„æˆã€‚ç½‘ç»œè¾“å‡º $17 \times 56 \times 56$ å…³é”®ç‚¹æ©ç å’Œ $25 \times 112 \times 112$ IUV å›¾ã€‚è¿™ä¸ªè¿‡ç¨‹åœ¨å›¾ 5 ä¸­è¢«è¯æ˜ã€‚åº”è¯¥æ³¨æ„çš„æ˜¯ï¼Œæ¨¡æ€è½¬æ¢ç½‘ç»œå’Œ WiFi-DensePose RCNN ä¸€èµ·è¢«è®­ç»ƒã€‚

## 3.4 Transfer Learning
Training the Modality Translation Network and WiFi-DensePose RCNN network from a random initialization takes a lot of time (roughly 80 hours). To improve the training efficiency, we conduct transfer learning from an image-based DensPose network to our WiFi-based network (See Figure 6 for details).

> ä»éšæœºåˆå§‹åŒ–è®­ç»ƒModality Translation Networkå’ŒWiFi-DensePose RCNNç½‘ç»œéœ€è¦å¤§é‡æ—¶é—´ï¼ˆå¤§çº¦80å°æ—¶ï¼‰ã€‚ä¸ºäº†æé«˜è®­ç»ƒæ•ˆç‡ï¼Œæˆ‘ä»¬ä»åŸºäºå›¾åƒçš„DensPoseç½‘ç»œè½¬ç§»å­¦ä¹ åˆ°æˆ‘ä»¬çš„åŸºäºWiFiçš„ç½‘ç»œï¼ˆè¯¦æƒ…è¯·å‚è§å›¾6ï¼‰ã€‚

The idea is to supervise the training of the WiFi-based network with the pre-trained image-based network. Directly initializing the WiFi-based network with image-based network weights does not work because the two networks get inputs from different domains (image and channel state information). Instead, we first train an image-based DensePose-RCNN model as a teacher network. Our student network consists of the modality translation network and the WiFi-DensePose RCNN. We fix the teacher network weights and train the student network by feeding them with the synchronized images and CSI tensors, respectively. We update the student network such that its backbone (ResNet) features mimic that of our teacher network. Our transfer learning goal is to minimize the differences of multiple levels of feature maps generated by the student model and those generated by the teacher model. Therefore we calculate the mean squared error between feature maps. The transfer learning loss from the teacher network to the student network is:

> è¯¥æƒ³æ³•æ˜¯ç›‘ç£WiFiåŸºç¡€ç½‘ç»œçš„è®­ç»ƒä¸é¢„å…ˆè®­ç»ƒçš„åŸºäºå›¾åƒçš„ç½‘ç»œã€‚ç›´æ¥ç”¨åŸºäºå›¾åƒçš„ç½‘ç»œæƒé‡åˆå§‹åŒ–åŸºäºWiFiçš„ç½‘ç»œä¸èµ·ä½œç”¨ï¼Œå› ä¸ºè¿™ä¸¤ä¸ªç½‘ç»œçš„è¾“å…¥æ¥è‡ªä¸åŒçš„åŸŸï¼ˆå›¾åƒå’Œä¿¡é“çŠ¶æ€ä¿¡æ¯ï¼‰ã€‚ç›¸åï¼Œæˆ‘ä»¬é¦–å…ˆå°†åŸºäºå›¾åƒçš„DensePose-RCNNæ¨¡å‹è®­ç»ƒä¸ºæ•™å¸ˆç½‘ç»œã€‚æˆ‘ä»¬çš„å­¦ç”Ÿç½‘ç»œåŒ…æ‹¬å˜æ€è½¬æ¢ç½‘ç»œå’ŒWiFi-DensePose RCNNã€‚æˆ‘ä»¬å›ºå®šæ•™å¸ˆç½‘ç»œçš„æƒé‡ï¼Œé€šè¿‡å‘å­¦ç”Ÿç½‘ç»œåˆ†åˆ«æä¾›åŒæ­¥çš„å›¾åƒå’ŒCSIå¼ é‡è¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬æ›´æ–°å­¦ç”Ÿç½‘ç»œï¼Œä½¿å…¶éª¨æ¶ï¼ˆResNetï¼‰ç‰¹å¾æ¨¡ä»¿æ•™å¸ˆç½‘ç»œã€‚æˆ‘ä»¬çš„è½¬ç§»å­¦ä¹ ç›®æ ‡æ˜¯æœ€å°åŒ–å­¦ç”Ÿæ¨¡å‹ç”Ÿæˆçš„å¤šä¸ªç‰¹å¾å›¾ä¸æ•™å¸ˆæ¨¡å‹ç”Ÿæˆçš„ç‰¹å¾å›¾ä¹‹é—´çš„å·®å¼‚ã€‚å› æ­¤ï¼Œæˆ‘ä»¬è®¡ç®—ç‰¹å¾å›¾ä¹‹é—´çš„å‡æ–¹è¯¯å·®ã€‚ä»æ•™å¸ˆç½‘ç»œåˆ°å­¦ç”Ÿç½‘ç»œçš„è½¬ç§»å­¦ä¹ æŸå¤±ä¸ºï¼š

**Eq. 3**

$$
L_{tr} = MSE(P_2, P_2^*) + MSE(P_3, P_3^*) + MSE(P_4, P_4^*) + MSE(P_5, P_5^*)
$$

where $ğ‘€ğ‘†ğ¸(\cdot)$ computes the mean squared error between two fea- ture maps, $\{ P_2, P_3, P_4, P_5 \}$ is a set of feature maps produced by the teacher network [14], and $\{ P_2^*, P_3^*, P_4^*, P_5^* \}$ is the set of feature maps produced by the student network [14].

> å…¶ä¸­ï¼Œ$MSSE(\cdot)$ è®¡ç®—ä¸¤ä¸ªç‰¹å¾å›¾ä¹‹é—´çš„å‡æ–¹è¯¯å·®ï¼Œ${P_2, P_3, P_4, P_5}$ æ˜¯ç”±æ•™å¸ˆç½‘ç»œ[14]ç”Ÿæˆçš„ä¸€ç»„ç‰¹å¾å›¾ï¼Œ${P_2^, P_3^, P_4^, P_5^}$ æ˜¯ç”±å­¦ç”Ÿç½‘ç»œ[14]ç”Ÿæˆçš„ä¸€ç»„ç‰¹å¾å›¾ã€‚

Benefiting from the additional supervision from the image-based model, the student network gets higher performance and takes fewer iterations to converge (Please see results in Table 5).

> å—åˆ°å›¾åƒæ¨¡å‹çš„é¢å¤–ç›‘ç£ï¼Œå­¦ç”Ÿç½‘ç»œè·å¾—äº†æ›´é«˜çš„æ€§èƒ½ï¼Œå¹¶ä¸”éœ€è¦æ›´å°‘çš„è¿­ä»£æ‰èƒ½æ”¶æ•›ï¼ˆè¯·å‚è§è¡¨5ä¸­çš„ç»“æœï¼‰ã€‚

## 3.5 Losses
The total loss of our approach is computed as:

$$
L = L_{cls} + L_{box} + \lambda_{dp} L_{dp} + \lambda_{kp} L_{kp} + \lambda_{tr} L_{tr}
$$


where $L_{cls}$, $L_{box}$, $L_{kp}$, $L_{Ltr}$ are losses for the person classification, bounding box regression, DensePose, keypoints, and transfer learning respectively. The classification loss $L_{cls}$ and the box regression loss $L_{box}$ are standard RCNN losses [9, 21]. The DensePose loss $L_{dp}$ [8] consists of several sub-components: (1) Cross-entropy loss for the coarse segmentation tasks. Each pixel is classified as either belonging to the background or one of the 24 human body regions. (2) Cross-entropy loss for body part classification and smooth L1 loss for UV coordinate regression. These losses are used to determine the exact coordinates of the pixels, i.e., 24 regressors are created to break the full human into small parts and parameterize each piece using a local two-dimensional UV coordinate system, that identifies the position UV nodes on this surface part.

> å…¶ä¸­ï¼Œ$L_{cls}$ã€$L_{box}$ã€$L_{kp}$ã€$L_{Ltr}$ åˆ†åˆ«ä»£è¡¨äººå‘˜åˆ†ç±»ã€è¾¹ç•Œæ¡†å›å½’ã€DensePoseã€å…³é”®ç‚¹ä»¥åŠè½¬ç§»å­¦ä¹ çš„æŸå¤±ã€‚åˆ†ç±»æŸå¤± $L_{cls}$ å’Œè¾¹ç•Œæ¡†å›å½’æŸå¤± $L_{box}$ æ˜¯æ ‡å‡†çš„ RCNN æŸå¤± [9,21]ã€‚DensePose æŸå¤± $L_{dp}$ [8] ç”±å‡ ä¸ªå­éƒ¨åˆ†ç»„æˆï¼š(1)ç²—ç•¥åˆ†å‰²ä»»åŠ¡çš„äº¤å‰ç†µæŸå¤±ã€‚æ¯ä¸ªåƒç´ è¢«åˆ†ç±»ä¸ºå±äºèƒŒæ™¯æˆ– 24 ä¸ªäººä½“åŒºåŸŸä¹‹ä¸€ã€‚(2)èº«ä½“éƒ¨ä½åˆ†ç±»çš„äº¤å‰ç†µæŸå¤±å’Œ UV åæ ‡å›å½’çš„å¹³æ»‘ L1 æŸå¤±ã€‚è¿™äº›æŸå¤±ç”¨äºç¡®å®šåƒç´ çš„ç²¾ç¡®åæ ‡ï¼Œå³ï¼Œåˆ›å»º 24 ä¸ªå›å½’å™¨ï¼Œå°†æ•´ä¸ªäººåˆ†æˆå°éƒ¨åˆ†ï¼Œå¹¶ä½¿ç”¨å±€éƒ¨äºŒç»´ UV åæ ‡ç³»å¯¹æ¯ä¸ªéƒ¨åˆ†è¿›è¡Œå‚æ•°åŒ–ï¼Œè¯¥åæ ‡ç³»è¯†åˆ«è¯¥è¡¨é¢éƒ¨åˆ†çš„ UV èŠ‚ç‚¹çš„ä½ç½®ã€‚
![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/46b9b3784a8041d3be1bcb8e257aeb6d.png#pic_center)
*Figure 5: WiFi-DensePose RCNN. The 3 Ã— 720 Ã— 1280 feature map from Figure 4 first goes through standard ResNet-FPN and ROI pooling to extract person-wise features. The features are then processed by two heads:the Keypoint Head and the DensePose Head.*

> å›¾5ï¼šWiFi-DensePose RCNNã€‚æ¥è‡ªå›¾4çš„3 Ã— 720 Ã— 1280ç‰¹å¾æ˜ å°„é¦–å…ˆç»è¿‡æ ‡å‡†çš„ResNet-FPNå’ŒROIæ± åŒ–ä»¥æå–äººå‘˜ç‰¹å¾ã€‚ç„¶åï¼Œè¿™äº›ç‰¹å¾è¢«ä¸¤ä¸ªå¤´éƒ¨å¤„ç†ï¼šå…³é”®ç‚¹å¤´å’Œå¯†é›†å§¿åŠ¿å¤´ã€‚

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/0e1c2dd8fbe34bc7bb4723066e7459ed.png#pic_center)

*Figure 6: Transfer learning from an image-based teacher network to our WiFi-based network.*

> å›¾ 6ï¼šä»åŸºäºå›¾åƒçš„æ•™å¸ˆç½‘ç»œè½¬ç§»åˆ°æˆ‘ä»¬åŸºäºWiFiçš„ç½‘ç»œã€‚

We add $L_{kp}$ to help the DensePose to balance between the torso with more UV nodes and limbs with fewer UV nodes. Inspired by Keypoint RCNN [9], we one-hot-encode each of the 17 ground truth keypoints in one $56 \times 56$ heatmap, generating $17 \times 56 \times 56$ keypoints heatmaps and supervise the output with the Cross-Entropy Loss. To closely regularize the Densepose regression, the keypoint heatmap regressor takes the same input features used by the Denspose UV maps.

> æˆ‘ä»¬æ·»åŠ $L_{kp}$æ¥å¸®åŠ©DensePoseå¹³è¡¡å…·æœ‰æ›´å¤šUVèŠ‚ç‚¹çš„èº¯å¹²å’Œå…·æœ‰è¾ƒå°‘UVèŠ‚ç‚¹çš„å››è‚¢ä¹‹é—´çš„å¹³è¡¡ã€‚å—Keypoint RCNN [9]çš„å¯å‘ï¼Œæˆ‘ä»¬åœ¨ä¸€ä¸ª$56 \times 56$çš„çƒ­å›¾ä¸­å¯¹17ä¸ªåŸºç¡€å…³é”®ç‚¹è¿›è¡Œä¸€æ¬¡çƒ­ç¼–ç ï¼Œç”Ÿæˆ$17 \times 56 \times 56$å…³é”®ç‚¹çƒ­å›¾å¹¶ä½¿ç”¨äº¤å‰ç†µæŸå¤±ç›‘ç£è¾“å‡ºã€‚ä¸ºäº†å¯†åˆ‡è§„èŒƒDenseposeå›å½’ï¼Œå…³é”®ç‚¹çƒ­å›¾å›å½’é‡‡ç”¨ä¸Densepose UVå›¾ä½¿ç”¨çš„ç›¸åŒè¾“å…¥ç‰¹å¾ã€‚

# 4 EXPERIMENTS
This section presents the experimental validation of our WiFi-based DensePose.

## 4.1 Dataset
We used the dataset[^1] described in [31], which contains CSI samples taken at 100Hz from receiver antennas and videos recorded at 20 FPS. Time stamps are used to synchronize CSI and the video frames such that 5 CSI samples correspond to 1 video frame. The dataset was gathered in sixteen spatial layouts: six captures in the lab office and ten captures in the classroom. Each capture is around 13 minutes with 1 to 5 subjects (8 subjects in total for the entire dataset) performing daily activities under the layout described in Figure 2 (a). **The sixteen spatial layouts are different in their relative locations/orientations of the WiFi-emitter antennas, person, furniture, and WiFi-receiver antennas.**

> æˆ‘ä»¬ä½¿ç”¨äº†åœ¨[31]ä¸­æè¿°çš„æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«ä»¥100Hzçš„é¢‘ç‡ä»æ¥æ”¶å¤©çº¿è·å–çš„CSIæ ·æœ¬å’Œä»¥20 FPSå½•åˆ¶çš„è§†é¢‘ã€‚ä½¿ç”¨æ—¶é—´æˆ³ä»¥åŒæ­¥CSIå’Œè§†é¢‘å¸§ï¼Œä½¿å¾—5ä¸ªCSIæ ·æœ¬å¯¹åº”äº1ä¸ªè§†é¢‘å¸§ã€‚è¯¥æ•°æ®é›†åœ¨åå…­ä¸ªç©ºé—´å¸ƒå±€ä¸­è¢«æ”¶é›†ï¼šå®éªŒå®¤åŠå…¬å®¤çš„å…­æ¬¡æ•æ‰å’Œæ•™å®¤çš„åæ¬¡æ•æ‰ã€‚æ¯æ¬¡æ•æ‰å¤§çº¦ä¸º13åˆ†é’Ÿï¼Œ1è‡³5ä¸ªç§‘ç›®ï¼ˆæ•´ä¸ªæ•°æ®é›†å…±8ä¸ªç§‘ç›®ï¼‰åœ¨å›¾2ï¼ˆaï¼‰æè¿°çš„å¸ƒå±€ä¸‹æ‰§è¡Œæ—¥å¸¸æ´»åŠ¨ã€‚åå…­ç§ç©ºé—´å¸ƒå±€åœ¨WiFiå‘å°„å¤©çº¿ï¼Œäººï¼Œå®¶å…·å’ŒWiFiæ¥æ”¶å¤©çº¿çš„ç›¸å¯¹ä½ç½®/æ–¹å‘ä¸åŒã€‚

There are no manual annotations for the data set. We use the MS-COCO-pre-trained dense model "R_101_FPN_s1x_legacy" [^2] and MS-COCO-pre-trained Keypoint R-CNN "R101-FPN" [^3] to pro- duce the pseudo ground truth. We denote the ground truth as "R101-Pseudo-GT" (see an annotated example in Figure 7). The R101-Pseudo-GT includes person bounding boxes, person-instance segmentation masks, body-part UV maps, and person-wise key- point coordinates.

> è¿™ä¸ªæ•°æ®é›†æ²¡æœ‰äººå·¥æ ‡æ³¨ã€‚æˆ‘ä»¬ä½¿ç”¨ä»¥MS-COCOæ•°æ®é›†é¢„è®­ç»ƒçš„å¯†é›†æ¨¡å‹ "R_101_FPN_s1x_legacy"[^2] å’Œ MS-COCO é¢„è®­ç»ƒçš„å…³é”®ç‚¹R-CNN "R101-FPN"[^3]ç”Ÿæˆä¼ªåœ°é¢å®å†µã€‚æˆ‘ä»¬å°†åœ°é¢å®å†µç§°ä¸º "R101-Pseudo-GT" (è¯·å‚è§å›¾7ä¸­çš„æ³¨é‡Šç¤ºä¾‹)ã€‚R101-Pseudo-GT åŒ…æ‹¬äººçš„è¾¹ç•Œæ¡†ã€äººä½“å®ä¾‹åˆ†å‰²æ©ç ã€èº«ä½“éƒ¨ä½çš„UVå›¾å’Œä¸ªäººå…³é”®ç‚¹åæ ‡ã€‚

Throughout the section, we use R101-Puedo-GT to train our WiFi-based DensePose model as well as finetuning the image-based baseline "R_50_FPN_s1x_legacy".

> åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨R101-Puedo-GTè®­ç»ƒæˆ‘ä»¬çš„åŸºäºWiFiçš„DensePoseæ¨¡å‹ï¼Œå¹¶å¾®è°ƒåŸºäºå›¾åƒçš„åŸºçº¿"R_50_FPN_s1x_legacy"ã€‚

[^1]: The identifiable information in this dataset has been removed for any privacy concerns.
[^2]: https://github.com/facebookresearch/detectron2/blob/main/projects/DensePose/ doc/DENSEPOSE_IUV.md#ModelZoo
[^3]: https://github.com/facebookresearch/detectron2/blob/main/MODEL_ZOO.md# coco- person- keypoint- detection- baselines- with- keypoint- r- cnn


![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/e4e8cc99386c4dc2b9f22d0b14ba7b3d.png#pic_center)
*Figure 7: Top two rows are the amplitude and phase of the input WiFi signal. The bottom row shows R101-Psuedo-GT: the ground truth dense pose and keypoints annotated by running a pretrained image-based Densepose network on the corresponding RGB image (see Section 4.1 for details).*

> å›¾ 7ï¼šé¡¶éƒ¨çš„ä¸¤è¡Œæ˜¯è¾“å…¥ WiFi ä¿¡å·çš„æŒ¯å¹…å’Œç›¸ä½ã€‚åº•è¡Œæ˜¾ç¤º R101-Pseudo-GTï¼šç”¨é¢„å…ˆè®­ç»ƒçš„åŸºäºå›¾åƒçš„ DensePose ç½‘ç»œåœ¨ç›¸åº” RGB å›¾åƒä¸Šè¿è¡Œåæ³¨é‡Šçš„å¯†é›†å§¿æ€å’Œå…³é”®ç‚¹çš„çœŸå€¼ï¼ˆæœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§ç¬¬ 4.1 èŠ‚ï¼‰ã€‚

## 4.2 Training/Testing protocols and Metrics
We report results on two protocols: (1) Same layout: We train on the training set in all 16 spatial layouts, and test on remaining frames. Following [31], we randomly select 80% of the samples to be our training set, and the rest to be our testing set. The training and testing samples are different in the personâ€™s location and pose, but share the same personâ€™s identities and background. This is a reasonable assumption since the WiFi device is usually installed in a fixed location. (2) **Different layout**: We train on 15 spatial layouts and test on 1 unseen spatial layout. The unseen layout is in the classroom scenarios.

> æˆ‘ä»¬æŠ¥å‘Šä¸¤ç§åè®®çš„ç»“æœï¼šï¼ˆ1ï¼‰ç›¸åŒå¸ƒå±€ï¼šæˆ‘ä»¬åœ¨æ‰€æœ‰16ç§ç©ºé—´å¸ƒå±€çš„è®­ç»ƒé›†ä¸Šè®­ç»ƒï¼Œå¹¶åœ¨å‰©ä½™å¸§ä¸Šè¿›è¡Œæµ‹è¯•ã€‚éµå¾ª[31]ï¼Œæˆ‘ä»¬éšæœºé€‰æ‹©80ï¼…çš„æ ·æœ¬ä½œä¸ºæˆ‘ä»¬çš„è®­ç»ƒé›†ï¼Œå…¶ä½™æ ·æœ¬ä½œä¸ºæˆ‘ä»¬çš„æµ‹è¯•é›†ã€‚è®­ç»ƒå’Œæµ‹è¯•æ ·æœ¬åœ¨äººçš„ä½ç½®å’Œå§¿æ€ä¸Šä¸åŒï¼Œä½†å…±äº«ç›¸åŒçš„äººçš„èº«ä»½å’ŒèƒŒæ™¯ã€‚è¿™æ˜¯ä¸€ä¸ªåˆç†çš„å‡è®¾ï¼Œå› ä¸ºWiFiè®¾å¤‡é€šå¸¸å®‰è£…åœ¨å›ºå®šä½ç½®ã€‚ï¼ˆ2ï¼‰ä¸åŒå¸ƒå±€ï¼šæˆ‘ä»¬åœ¨15ç§ç©ºé—´å¸ƒå±€ä¸Šè®­ç»ƒï¼Œåœ¨1ä¸ªçœ‹ä¸è§çš„ç©ºé—´å¸ƒå±€ä¸Šæµ‹è¯•ã€‚æœªè§çš„å¸ƒå±€åœ¨æ•™å®¤æƒ…æ™¯ä¸­ã€‚

We evaluate the performance of our algorithm in two aspects: the ability to detect humans (bounding boxes) and accuracy of the dense pose estimation.

> æˆ‘ä»¬è¯„ä¼°ç®—æ³•åœ¨ä¸¤æ–¹é¢çš„è¡¨ç°ï¼šæ£€æµ‹äººç±»ï¼ˆè¾¹ç•Œæ¡†ï¼‰çš„èƒ½åŠ›å’Œå¯†é›†å§¿æ€ä¼°è®¡çš„å‡†ç¡®æ€§ã€‚

To evaluate the performance of our models in detecting humans, we calculate the standard average precision (AP) of person bounding boxes at multiple IOU thresholds ranging from 0.5 to 0.95.

> ä¸ºäº†è¯„ä¼°æ¨¡å‹åœ¨æ£€æµ‹äººç±»æ–¹é¢çš„æ€§èƒ½ï¼Œæˆ‘ä»¬è®¡ç®—äººç±»è¾¹ç•Œæ¡†çš„æ ‡å‡†å¹³å‡ç²¾åº¦ï¼ˆAPï¼‰ï¼Œå…¶IOUé˜ˆå€¼ä»0.5åˆ°0.95ä¸ç­‰ã€‚

In addition, by MS-COCO [15] definition, we also compute AP-m for median bodies that are enclosed in bounding boxes with sizes between $32 \times 32$ and $96 \times 96$ pixels in a normalized $640 \times 480$ pixels image space, and AP-l for large bodies that are enclosed in bounding boxes larger than $96 \times 96$ pixels.

> æ­¤å¤–ï¼Œæ ¹æ® MS-COCO [15] çš„å®šä¹‰ï¼Œæˆ‘ä»¬è¿˜è®¡ç®— AP-mï¼Œè¿™æ˜¯æŒ‡ä»¥ $32 \times 32$ åˆ° $96 \times 96$ åƒç´ çš„å¤§å°åŒ…å«åœ¨ $640 \times 480$ åƒç´ çš„å½’ä¸€åŒ–å›¾åƒç©ºé—´ä¸­çš„è¾¹ç•Œæ¡†ä¸­çš„ä¸­ä½æ•°èº«ä½“ï¼›AP-l æ˜¯æŒ‡è¾¹ç•Œæ¡†å¤§äº $96 \times 96$ åƒç´ çš„å¤§å‹èº«ä½“ã€‚

To measure the performance of DensePose detection, we follow the original DensePose paper [8]. We first compute Geodesic Point Similarity (GPS) as a matching score for dense correspondences:

> ä¸ºäº†è¡¡é‡ DensePose æ£€æµ‹çš„æ€§èƒ½ï¼Œæˆ‘ä»¬éµå¾ª DensePose åŸå§‹è®ºæ–‡ [8]ã€‚æˆ‘ä»¬é¦–å…ˆè®¡ç®—å‡ ä½•ç‚¹ç›¸ä¼¼æ€§ï¼ˆGPSï¼‰ä½œä¸ºå¯†é›†å¯¹åº”çš„åŒ¹é…åˆ†æ•°ï¼š

**E.q. 4**

$$
GPS_{j} = \frac{1}{| P_j |} \sum_{p \in P_j} \exp (\frac{ -g (i_p, \hat{i}_p)^2 }{2 k^2})
$$

where $g$ calculates the geodesic distance, $P_j$ denotes the ground truth point annotations of person $j$, $i_p$ and $\hat{i}_p$ are the estimated and ground truth vertex at point $p$ respectively, and $k$ is a normalizing parameter (set to be 0.255 according to [8]).

> $g$ è®¡ç®—å‡ ä½•è·ç¦»ï¼Œ$P_j$ è¡¨ç¤ºäºº $j$ çš„åœ°é¢çœŸå®ç‚¹æ³¨é‡Šï¼Œ$i_p$ å’Œ $\hat{i}_p$ åˆ†åˆ«æ˜¯ç‚¹ $p$ çš„ä¼°è®¡å€¼å’Œåœ°é¢å®å†µå€¼ï¼Œ$k$ æ˜¯å½’ä¸€åŒ–å‚æ•°ï¼ˆæ ¹æ® [8] è®¾ä¸º 0.255ï¼‰ã€‚

One issue of GPS is that it does not penalize spurious predictions. Therefore, estimations with all pixels classified as foreground are favored. To alleviate this issue, masked geodesic point similarity (GPSm) was introduced in [8], which incorporates both GPS and segmentation masks. The formulation is as follows:

> GPS çš„ä¸€ä¸ªé—®é¢˜æ˜¯å®ƒä¸æƒ©ç½šè¯¯æŠ¥é¢„æµ‹ã€‚å› æ­¤ï¼Œå°†æ‰€æœ‰åƒç´ åˆ†ç±»ä¸ºå‰æ™¯çš„ä¼°è®¡å€¼å—åˆ°é’çã€‚ä¸ºäº†å‡è½»è¿™ä¸ªé—®é¢˜ï¼Œåœ¨ [8] ä¸­å¼•å…¥äº†å¸¦æ©ç çš„å‡ ä½•ç‚¹ç›¸ä¼¼åº¦ï¼ˆGPSmï¼‰ï¼Œå®ƒç»“åˆäº† GPS å’Œåˆ†å‰²æ©ç ã€‚å…¬å¼å¦‚ä¸‹ï¼š

**E.q. 5**

$$
GPSm = \sqrt{GPS \cdot I}, I = \frac{M \cap \hat M}{M \cup \hat M}
$$

where $M$ and $\hat M$ are the predicted and ground truth foreground segmentation masks.

> å…¶ä¸­ $M$ å’Œ $\hat M$ æ˜¯é¢„æµ‹çš„å’ŒçœŸå®å‰æ™¯åˆ†å‰²æ©ç ã€‚

Next, we can calculate DensePose average precision with GPS (denoted as $dpAP \cdot GPS$) and GPSm (denoted as $dpAP \cdot GPSm$) as thresholds, following the same logic behind the calculation of bounding box AP.

> æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨GPSï¼ˆè¡¨ç¤ºä¸º$dpAP \cdot GPS$ï¼‰å’ŒGPSmï¼ˆè¡¨ç¤ºä¸º$dpAP \cdot GPSm$ï¼‰ä½œä¸ºé˜ˆå€¼è®¡ç®—DensePoseå¹³å‡ç²¾åº¦ï¼Œéµå¾ªä¸è®¡ç®—è¾¹ç•Œæ¡†APç›¸åŒçš„é€»è¾‘ã€‚

## 4.3 Implementation Details

We implemented our approach in PyTorch. We set the training batch size to 16 on a 4 GPU (Titan X) server. We empirically set $\lambda_{dp} = 0.6$, $\lambda_{kp} = 0.3$, $\lambda_{tr} = 0.1$. We used a warmup multi-step learning rate scheduler and set the initial learning rate as $1e âˆ’ 5$. The learning rate increases to $1e âˆ’ 3$ during the first 2000 iterations then decreases to $\frac{1}{10}$ of its value every 48000 iterations. We trained for 145, 000 iterations for our final model.

> æˆ‘ä»¬ä½¿ç”¨ PyTorch å®ç°äº†æˆ‘ä»¬çš„æ–¹æ³•ã€‚æˆ‘ä»¬åœ¨ 4 GPUï¼ˆTitan Xï¼‰æœåŠ¡å™¨ä¸Šå°†è®­ç»ƒæ‰¹æ¬¡è®¾ç½®ä¸º 16ã€‚æˆ‘ä»¬ç»éªŒè¯åœ°è®¾ç½®äº† $\lambda_{dp} = 0.6$ï¼Œ$\lambda_{kp} = 0.3$ï¼Œ$\lambda_{tr} = 0.1$ã€‚æˆ‘ä»¬ä½¿ç”¨äº†ä¸€ä¸ªçƒ­èº«å¤šæ­¥å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼Œå¹¶å°†åˆå§‹å­¦ä¹ ç‡è®¾ä¸º $1e âˆ’ 5$ã€‚å­¦ä¹ ç‡åœ¨å‰ 2000 æ¬¡è¿­ä»£ä¸­å¢åŠ åˆ° $1e âˆ’ 3$ï¼Œç„¶åæ¯ 48000 æ¬¡è¿­ä»£å‡å°‘åˆ°å…¶å€¼çš„ $\frac{1}{10}$ã€‚æˆ‘ä»¬ä¸ºæœ€ç»ˆæ¨¡å‹è®­ç»ƒäº† 145,000 æ¬¡è¿­ä»£ã€‚

## 4.4 WiFi-based DensePose under Same Layout
Under the Same Layout protocol, we compute the AP of human bounding box detections as well as $dpAP \cdot GPS$ and $dpAP \cdot GPSm$ of dense correspondence predictions. Results are presented in Table 1 and Table 2, respectively.

> åœ¨â€œåŒä¸€å¸ƒå±€â€åè®®ä¸‹ï¼Œæˆ‘ä»¬è®¡ç®—äººä½“è¾¹ç•Œæ¡†æ£€æµ‹çš„APä»¥åŠå¯†é›†å¯¹åº”é¢„æµ‹çš„ $dpAP \cdot GPS$å’Œ $dpAP \cdot GPSm$ã€‚ç»“æœåˆ†åˆ«åœ¨è¡¨1å’Œè¡¨2ä¸­å‘ˆç°ã€‚

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/04e4ed02428943cb85ea73161e0b5c41.png#pic_center)
*Table 1: Average precision (AP) of WiFi-based DensePose un- der the Same Layout protocol. All metrics are the higher the better.*

> è¡¨1ï¼šåœ¨åŒä¸€å¸ƒå±€åè®®ä¸‹WiFiåŸºç¡€å¯†é›†å§¿æ€çš„å¹³å‡ç²¾åº¦ï¼ˆAPï¼‰ã€‚æ‰€æœ‰æŒ‡æ ‡è¶Šé«˜è¶Šå¥½ã€‚

From Table 1, we can observe a high value (87.2) of AP@50, indicating that our model can effectively detect the approximate locations of human bounding boxes. The relatively low value (35.6) for AP@75 suggests that the details of the human bodies are not perfectly estimated.

> ä»è¡¨1å¯ä»¥çœ‹å‡ºï¼ŒAP@50çš„å€¼è¾ƒé«˜ï¼ˆ87.2ï¼‰ï¼Œè¡¨æ˜æˆ‘ä»¬çš„æ¨¡å‹å¯ä»¥æœ‰æ•ˆåœ°æ£€æµ‹äººä½“è¾¹ç•Œæ¡†çš„å¤§è‡´ä½ç½®ã€‚ AP@75çš„ç›¸å¯¹è¾ƒä½çš„å€¼ï¼ˆ35.6ï¼‰è¡¨æ˜äººä½“çš„ç»†èŠ‚æ²¡æœ‰å¾—åˆ°å®Œç¾çš„ä¼°è®¡ã€‚

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/41866f5e740b44b0a91e7942bc6184f9.png#pic_center)

*Table 2: DensePose Average precision (dpAP $\cdot$ GPS, dpAP $\cdot$ GPSm) of WiFi-based DensePose under the Same Layout protocol. All metrics are the higher the better.*

> è¡¨2ï¼šWiFiåŸºç¡€DensePoseåœ¨åŒä¸€å¸ƒå±€åè®®ä¸‹çš„å¯†é›†å¯¹åº”çš„å¹³å‡ç²¾åº¦ï¼ˆdpAPÂ·GPSï¼ŒdpAPÂ·GPSmï¼‰ã€‚æ‰€æœ‰æŒ‡æ ‡è¶Šé«˜è¶Šå¥½ã€‚

A similar pattern can be seen from the results of DensePose estimations (see Table 2 for details). Experiments report high values of dpAP $\cdot$ GPS@50 and dpAP $\cdot$ GPSm@50, but low values of dpAP $\cdot$ GPS@75 and dpAP $\cdot$ GPSm@75. This demonstrates that our model performs well at estimating the poses of human torsos, but still struggles with detecting details like limbs.

> ä»ç»“æœä¸­ä¹Ÿå¯ä»¥çœ‹å‡ºç±»ä¼¼çš„æ¨¡å¼ï¼ˆè¯¦æƒ…è¯·è§è¡¨2ï¼‰ã€‚å®éªŒæŠ¥å‘Šäº†é«˜çš„dpAP $\cdot$ GPS@50å’ŒdpAP $\cdot$ GPSm@50å€¼ï¼Œä½†dpAP $\cdot$ GPS@75å’ŒdpAP $\cdot$ GPSm@75çš„å€¼è¾ƒä½ã€‚è¿™è¡¨æ˜æˆ‘ä»¬çš„æ¨¡å‹åœ¨ä¼°è®¡äººä½“èº¯å¹²çš„å§¿æ€æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†ä»ç„¶åœ¨æ£€æµ‹åƒå››è‚¢è¿™æ ·çš„ç»†èŠ‚æ–¹é¢å­˜åœ¨å›°éš¾ã€‚

## 4.5 Comparison with Image-based DensePose

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/000533281df7454ba5a3e2ca13347989.png#pic_center)
*Table 3: Average precision (AP) of WiFi-based and image- based DensePose under the Same Layout protocol. All met- rics are the higher the better.*

> è¡¨3ï¼šåœ¨ç›¸åŒå¸ƒå±€åè®®ä¸‹WiFiåŸºç¡€å’Œå›¾åƒåŸºç¡€çš„DensePoseçš„å¹³å‡ç²¾åº¦ï¼ˆAPï¼‰ã€‚æ‰€æœ‰æŒ‡æ ‡è¶Šé«˜è¶Šå¥½ã€‚

As discussed in Section 4.1, since there are no manual annota- tions on the WiFi dataset, it is difficult to compare the performance of WiFi-based DensePose with its Image-based counterpart. This is a common drawback of many WiFi perception works including [31].

> å¦‚4.1èŠ‚ä¸­æ‰€è®¨è®ºçš„ï¼Œç”±äºWiFiæ•°æ®é›†ä¸Šæ²¡æœ‰æ‰‹åŠ¨æ³¨é‡Šï¼Œå› æ­¤å¾ˆéš¾å°†WiFiåŸºç¡€çš„DensePoseä¸å…¶åŸºäºå›¾åƒçš„å¯¹åº”ç‰©è¿›è¡Œæ¯”è¾ƒã€‚è¿™æ˜¯è®¸å¤šWiFiæ„ŸçŸ¥å·¥ä½œçš„å¸¸è§ç¼ºç‚¹ï¼ŒåŒ…æ‹¬[31]ã€‚

Nevertheless, conducting such a comparison is still worthwhile in assessing the current limit of WiFi-based perception. We tried an image-based DensePose baseline "R_50_FPN_s1x_legacy" finetuned on R101-Pseudo-GT under the Same Layout protocol. In addition, as shown in Figure 9 and Figure 10, though certain defects still exist, the estimations from our WiFi-based model are reasonably well compared to the results produced by Image-based DensePose.

> å°½ç®¡å¦‚æ­¤ï¼Œè¿›è¡Œè¿™æ ·çš„æ¯”è¾ƒä»ç„¶æœ‰æ„ä¹‰ï¼Œå› ä¸ºå®ƒå¯ä»¥è¯„ä¼°WiFiåŸºç¡€æ„ŸçŸ¥çš„å½“å‰é™åˆ¶ã€‚æˆ‘ä»¬è¯•å›¾åœ¨Same Layoutåè®®ä¸‹å¯¹åŸºäºå›¾åƒçš„DensePoseåŸºçº¿"R_50_FPN_s1x_legacy"è¿›è¡Œå¾®è°ƒï¼Œè¯¥åŸºçº¿åœ¨R101-Pseudo-GTä¸Šè¿›è¡Œäº†å¾®è°ƒã€‚æ­¤å¤–ï¼Œå¦‚å›¾9å’Œå›¾10æ‰€ç¤ºï¼Œå°½ç®¡ä»å­˜åœ¨æŸäº›ç¼ºé™·ï¼Œä½†æˆ‘ä»¬çš„åŸºäºWiFiçš„æ¨¡å‹çš„ä¼°è®¡ä¸åŸºäºå›¾åƒçš„DensePoseçš„ç»“æœç›¸æ¯”æ˜¯åˆç†çš„ã€‚

In the quantitative results in Table 3 and Table 4, the image-based baseline produces very high APs due to the small difference between its ResNet50 backbone and the Resnet101 backbone used to generate R101-Pseudo-GT. This is to be expected. Our WiFi-based model has much lower absolute metrics. However, it can be observed from Table 3 that the difference between AP-m and AP-l values is relatively small for the WiFi-based model. We believe this is because individuals who are far away from cameras occupy less space in the image, which leads to less information about these subjects. On the contrary, WiFi signals incorporate all the information in the entire scene, regardless of the subjectsâ€™ locations.

> åœ¨è¡¨3å’Œè¡¨4çš„å®šé‡ç»“æœä¸­ï¼ŒåŸºäºå›¾åƒçš„åŸºçº¿ç”±äºå…¶ResNet50éª¨å¹²ä¸ç”¨äºç”ŸæˆR101-Pseudo-GTçš„Resnet101éª¨å¹²çš„å·®å¼‚å¾ˆå°ï¼Œå› æ­¤äº§ç”Ÿäº†éå¸¸é«˜çš„APã€‚è¿™æ˜¯å¯ä»¥é¢„æœŸçš„ã€‚æˆ‘ä»¬çš„åŸºäºWiFiçš„æ¨¡å‹å…·æœ‰æ›´ä½çš„ç»å¯¹æŒ‡æ ‡ã€‚ç„¶è€Œï¼Œä»è¡¨3å¯ä»¥çœ‹å‡ºï¼ŒWiFiæ¨¡å‹çš„AP-må’ŒAP-lå€¼ä¹‹é—´çš„å·®å¼‚ç›¸å¯¹è¾ƒå°ã€‚æˆ‘ä»¬ç›¸ä¿¡è¿™æ˜¯å› ä¸ºè¿œç¦»æ‘„åƒå¤´çš„ä¸ªä½“å ç”¨å›¾åƒç©ºé—´è¾ƒå°ï¼Œå¯¼è‡´å¯¹è¿™äº›å¯¹è±¡ä¿¡æ¯è¾ƒå°‘ã€‚ç›¸åï¼ŒWiFiä¿¡å·åŒ…å«æ•´ä¸ªåœºæ™¯ä¸­çš„æ‰€æœ‰ä¿¡æ¯ï¼Œè€Œä¸è€ƒè™‘ä¸»ä½“çš„ä½ç½®ã€‚

## 4.6 Ablation Study

This section describes the ablation study to understand the effects of phase information, keypoint supervision, and transfer learning on estimating dense correspondences. Similar to section 4.4, the models analyzed in this section are all trained under the same-layout mentioned in section 4.2.

> è¿™ä¸€èŠ‚æè¿°äº†ä¸€ä¸ªæ¶ˆèç ”ç©¶ï¼Œä»¥äº†è§£ç›¸ä½ä¿¡æ¯ã€å…³é”®ç‚¹ç›‘ç£å’Œè½¬ç§»å­¦ä¹ å¯¹ä¼°è®¡å¯†é›†å¯¹åº”çš„å½±å“ã€‚ä¸ç¬¬4.4èŠ‚ç±»ä¼¼ï¼Œè¿™ä¸€èŠ‚åˆ†æçš„æ¨¡å‹å‡åœ¨ç¬¬4.2èŠ‚ä¸­æ‰€è¿°çš„ç›¸åŒå¸ƒå±€ä¸‹è®­ç»ƒã€‚

We start by training a baseline WiFi model, which does not in- clude the phase encoder, the keypoint detection branch, or transfer learning. The results are presented in the first row of both Table 5 and Table 6 as a reference.

> æˆ‘ä»¬é¦–å…ˆè®­ç»ƒä¸€ä¸ªåŸºçº¿WiFiæ¨¡å‹ï¼Œå®ƒä¸åŒ…æ‹¬ç›¸ä½ç¼–ç å™¨ã€å…³é”®ç‚¹æ£€æµ‹åˆ†æ”¯æˆ–è½¬ç§»å­¦ä¹ ã€‚ç»“æœåœ¨è¡¨5å’Œè¡¨6çš„ç¬¬ä¸€è¡Œå‘ˆç°ï¼Œä½œä¸ºå‚è€ƒã€‚

**Addition of Phase information:** We first examine whether the phase information can enhance the baseline performance. As shown in the second row of Table 5 and Table 6, the results for all the metrics have slightly improved from the baseline. This proves our hypothesis that the phase can reveal relevant information about the dense human pose.

> **ç›¸ä½ä¿¡æ¯çš„åŠ å…¥**: æˆ‘ä»¬é¦–å…ˆæ£€æŸ¥ç›¸ä½ä¿¡æ¯æ˜¯å¦èƒ½å¢å¼ºåŸºçº¿è¡¨ç°ã€‚å¦‚è¡¨5å’Œè¡¨6çš„ç¬¬äºŒè¡Œæ‰€ç¤ºï¼Œæ‰€æœ‰åº¦é‡çš„ç»“æœéƒ½ä»åŸºçº¿æœ‰æ‰€æé«˜ã€‚è¿™è¯æ˜äº†æˆ‘ä»¬çš„å‡è®¾ï¼Œå³ç›¸ä½å¯ä»¥æ­ç¤ºå…³äºå¯†é›†äººä½“å§¿åŠ¿çš„ç›¸å…³ä¿¡æ¯ã€‚

**Addition of a keypoint detection branch**: Having established the advantage of incorporating phase information, we now evaluate the effect of adding a keypoint branch to our model. The quantita- tive results are summarized in the third row of Table 5 and Table 6.

> **åŠ å…¥å…³é”®ç‚¹æ£€æµ‹åˆ†æ”¯**: åœ¨è¯æ˜äº†ç»“åˆç›¸ä½ä¿¡æ¯çš„ä¼˜åŠ¿ä¹‹åï¼Œæˆ‘ä»¬ç°åœ¨è¯„ä¼°åŠ å…¥å…³é”®ç‚¹åˆ†æ”¯å¯¹æˆ‘ä»¬çš„æ¨¡å‹çš„å½±å“ã€‚å®šé‡ç»“æœæ€»ç»“åœ¨è¡¨5å’Œè¡¨6çš„ç¬¬ä¸‰è¡Œä¸­ã€‚

Comparing with the numbers on the second row, we can observe a slight increase in performance in terms of dpAP $\cdot$ GPS@50(from 77.4 to 78.8) and dpAP $\cdot$ GPSm@50 (from 75.7 to 76.8), and a more noticeable improvement in terms of dpAP $\cdot$ GPS@75 (from 42.3 to 46.9) and dpAP $\cdot$ GPSm@75 (from 40.5 to 44.9). This indicates that the keypoint branch provides effective references to dense pose estimations, and our model becomes significantly better at detecting subtle details (such as the limbs).

> å¯¹æ¯”ç¬¬äºŒè¡Œçš„æ•°å­—ï¼Œæˆ‘ä»¬å¯ä»¥è§‚å¯Ÿåˆ°åœ¨ dpAP $\cdot$ GPS@50ï¼ˆä»77.4æé«˜åˆ°78.8ï¼‰å’ŒdpAP $\cdot$ GPSm@50ï¼ˆä»75.7æé«˜åˆ°76.8ï¼‰æ–¹é¢çš„æ€§èƒ½ç•¥å¾®æœ‰æ‰€æé«˜ï¼Œåœ¨dpAP $\cdot$ GPS@75ï¼ˆä»42.3æé«˜åˆ°46.9ï¼‰å’ŒdpAP $\cdot$ GPSm@75ï¼ˆä»40.5æé«˜åˆ°44.9ï¼‰æ–¹é¢çš„æé«˜åˆ™æ›´åŠ æ˜¾è‘—ã€‚è¿™è¡¨æ˜ï¼Œå…³é”®ç‚¹åˆ†æ”¯ä¸ºå¯†é›†å§¿åŠ¿ä¼°è®¡æä¾›äº†æœ‰æ•ˆçš„å‚è€ƒï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨æ£€æµ‹ç»†å¾®çš„ç»†èŠ‚ï¼ˆå¦‚å››è‚¢ï¼‰æ–¹é¢å˜å¾—æ›´å¥½ã€‚

**Effect of Transfer Learning**: We aim to reduce the training time for our model with the help of transfer learning. For each model in Table 5, we continue training the model until there are no significant changes in terms of performance. The last row of Table 5 and Table 6 represents our final model with transfer learn- ing. Though the final performance does not improve too much compared to the model (with phase information and keypoints) without transfer learning, it should be noted that the number of training iterations decreases significantly from 186000 to 145000 (this number includes the time to perform transfer learning as well as training the main model).

> **è½¬ç§»å­¦ä¹ çš„æ•ˆæœ**: æˆ‘ä»¬å¸Œæœ›å€ŸåŠ©è½¬ç§»å­¦ä¹ æ¥ç¼©çŸ­æˆ‘ä»¬æ¨¡å‹çš„è®­ç»ƒæ—¶é—´ã€‚å¯¹äºè¡¨5ä¸­çš„æ¯ä¸ªæ¨¡å‹ï¼Œæˆ‘ä»¬ç»§ç»­è®­ç»ƒæ¨¡å‹ï¼Œç›´åˆ°åœ¨æ€§èƒ½æ–¹é¢æ²¡æœ‰æ˜¾è‘—çš„å˜åŒ–ã€‚è¡¨5å’Œè¡¨6çš„æœ€åä¸€è¡Œä»£è¡¨æˆ‘ä»¬çš„æœ€ç»ˆæ¨¡å‹ï¼Œè¿›è¡Œè½¬ç§»å­¦ä¹ ã€‚è™½ç„¶ä¸ä¸è¿›è¡Œè½¬ç§»å­¦ä¹ çš„æ¨¡å‹ï¼ˆå¸¦æœ‰ç›¸ä½ä¿¡æ¯å’Œå…³é”®ç‚¹ï¼‰ç›¸æ¯”ï¼Œæœ€ç»ˆçš„è¡¨ç°æ²¡æœ‰å¤ªå¤§çš„æ”¹å–„ï¼Œä½†æ˜¯åº”è¯¥æ³¨æ„çš„æ˜¯ï¼Œè®­ç»ƒæ¬¡æ•°ä»186000æ˜¾ç€å‡å°‘åˆ°145000ï¼ˆè¿™ä¸ªæ•°å­—åŒ…æ‹¬æ‰§è¡Œè½¬ç§»å­¦ä¹ å’Œè®­ç»ƒä¸»æ¨¡å‹çš„æ—¶é—´ï¼‰ã€‚

## 4.7 Performance in different layouts

All above results are obtained using the same layout for training and testing. However, WiFi signals in different environments exhibit significantly different propagation patterns. Therefore, it is still a very challenging problem to deploy our model on data from an untrained layout.

> æ‰€æœ‰ä¸Šè¿°ç»“æœéƒ½æ˜¯åœ¨ä½¿ç”¨ç›¸åŒçš„å¸ƒå±€è¿›è¡Œè®­ç»ƒå’Œæµ‹è¯•æ—¶è·å¾—çš„ã€‚ä½†æ˜¯ï¼Œåœ¨ä¸åŒç¯å¢ƒä¸­çš„WiFiä¿¡å·è¡¨ç°å‡ºæ˜¾è‘—ä¸åŒçš„ä¼ æ’­æ¨¡å¼ã€‚å› æ­¤ï¼Œåœ¨æœªç»è®­ç»ƒçš„å¸ƒå±€çš„æ•°æ®ä¸Šéƒ¨ç½²æˆ‘ä»¬çš„æ¨¡å‹ä»ç„¶æ˜¯ä¸€ä¸ªéå¸¸å…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ã€‚

To test the robustness of our model, we conducted the previous experiment under the different layout protocols, where there are 15 training layouts and 1 testing layout. The experimental results are recorded in Table 7 and Table 8.

> ä¸ºäº†æµ‹è¯•æˆ‘ä»¬æ¨¡å‹çš„é²æ£’æ€§ï¼Œæˆ‘ä»¬åœ¨ä¸åŒçš„å¸ƒå±€åè®®ä¸‹è¿›è¡Œäº†å‰é¢çš„å®éªŒï¼Œå…¶ä¸­æœ‰15ä¸ªè®­ç»ƒå¸ƒå±€å’Œ1ä¸ªæµ‹è¯•å¸ƒå±€ã€‚å®éªŒç»“æœè®°å½•åœ¨è¡¨7å’Œè¡¨8ä¸­ã€‚

We can observe that our final model performs better than the baseline model in the unseen domain, but the performance decreases significantly from that under the same layout protocol: the AP performance drops from 43.5 to 27.3 and dpAP $\cdot$ GPS drops from 45.3 to 25.4. However, it should also be noted that the image-based model suffers from the same domain generalization problem. We believe a more comprehensive dataset from a wide range of settings can alleviate this issue.

> æˆ‘ä»¬å¯ä»¥å‘ç°ï¼Œæˆ‘ä»¬çš„æœ€ç»ˆæ¨¡å‹åœ¨çœ‹ä¸è§çš„é¢†åŸŸä¸­çš„è¡¨ç°æ¯”åŸºçº¿æ¨¡å‹å¥½ï¼Œä½†æ˜¯æ€§èƒ½ä»ç„¶æ˜æ˜¾ä¸‹é™ï¼šAPæ€§èƒ½ä»43.5é™åˆ°27.3ï¼ŒdpAP $\cdot$ GPSä»45.3é™åˆ°25.4ã€‚ä½†æ˜¯ï¼Œåº”è¯¥ä¹Ÿè¦æ³¨æ„çš„æ˜¯ï¼ŒåŸºäºå›¾åƒçš„æ¨¡å‹ä¹Ÿå­˜åœ¨ç›¸åŒçš„é¢†åŸŸæ³›åŒ–é—®é¢˜ã€‚æˆ‘ä»¬ç›¸ä¿¡æ¥è‡ªå„ç§è®¾ç½®çš„æ›´å…¨é¢çš„æ•°æ®é›†å¯ä»¥ç¼“è§£è¿™ä¸ªé—®é¢˜ã€‚

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/51713ddffd7e40c786a9403fb61ae3d5.png#pic_center)
*Table 4: DensePose Average precision (dpAP $\cdot$ GPS, dpAP $\cdot$ GPSm) of WiFi-based and image-based DensePose under the Same Layout protocol. All metrics are the higher the better.*

> *è¡¨4ï¼šç›¸åŒå¸ƒå±€åè®®ä¸‹åŸºäºWiFiå’ŒåŸºäºå›¾åƒçš„DensePoseçš„å¹³å‡ç²¾åº¦ï¼ˆdpAP $\cdot$ GPSï¼ŒdpAP $\cdot$ GPSmï¼‰ã€‚æ‰€æœ‰æŒ‡æ ‡éƒ½æ˜¯è¶Šé«˜è¶Šå¥½*ã€‚

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/b858a85a99c8447987d41ca8a99cba5f.png#pic_center)

*Table 5: Ablation study of human detection under the Same-layout protocol. All metrics are the higher the better.*

> *è¡¨5ï¼šç›¸åŒå¸ƒå±€æ–¹æ¡ˆä¸‹çš„äººç±»æ£€æµ‹çš„æ¶ˆèç ”ç©¶ã€‚æ‰€æœ‰æŒ‡æ ‡éƒ½æ˜¯è¶Šé«˜è¶Šå¥½*ã€‚

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/3eab382cd9a5478698793ee0a876a0dc.png#pic_center)
*Table 6: Ablation study of DensePose estimation under the Same-layout protocol. All metrics are the higher the better.*

> *è¡¨6ï¼šç›¸åŒå¸ƒå±€åè®®ä¸‹çš„DensePoseä¼°è®¡çš„æ¶ˆèç ”ç©¶ã€‚æ‰€æœ‰æŒ‡æ ‡éƒ½æ˜¯è¶Šé«˜è¶Šå¥½*ã€‚

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/927f16c65e624970bfa3a64f085aca94.png#pic_center)
*Table 7: Average precision (AP) of WiFi-based and image-based DensePose under the Different Layout protocol. All metrics are the higher the better.*

> *è¡¨7ï¼šä¸åŒå¸ƒå±€åè®®ä¸‹åŸºäºWiFiå’ŒåŸºäºå›¾åƒçš„DensePoseçš„å¹³å‡ç²¾åº¦ï¼ˆAPï¼‰ã€‚æ‰€æœ‰æŒ‡æ ‡éƒ½æ˜¯è¶Šé«˜è¶Šå¥½*ã€‚

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/6ad7c637bc894d0a940ae6785f0657a7.png#pic_center)
*Table 8: DensePose Average precision (dpAP $\cdot$ GPS, dpAP $\cdot$ GPSm) of WiFi-based and image-based DensePose under the Differ- ent Layout protocol. All metrics are the higher the better.*

> *è¡¨8ï¼šDensePose åœ¨Differ-ent Layoutåè®®ä¸‹ï¼ŒåŸºäºWiFiå’ŒåŸºäºå›¾åƒçš„DensePoseçš„å¹³å‡ç²¾åº¦ï¼ˆdpAP $\cdot$ GPS, dpAP $\cdot$ GPSmï¼‰ã€‚æ‰€æœ‰æŒ‡æ ‡éƒ½æ˜¯è¶Šé«˜è¶Šå¥½*ã€‚

## 4.8 Failure cases
We observed two main types of failure cases. (1) When there are body poses that rarely occurred in the training set, the WiFi-based model is biased and is likely to produce wrong body parts (See examples (a-b) in Figure 8). (2) When there are three or more concurrent subjects in one capture, it is more challenging for the WiFi-based model to extract detailed information for each individual from the amplitude and phase tensors of the entire capture. (See examples (c-d) in Figure 8). We believe both of these issues can be resolved by obtaining more comprehensive training data.

> æˆ‘ä»¬è§‚å¯Ÿåˆ°äº†ä¸¤ç§ä¸»è¦çš„å¤±è´¥æƒ…å†µã€‚ï¼ˆ1ï¼‰å½“è®­ç»ƒé›†ä¸­å¾ˆå°‘å‡ºç°èº«ä½“å§¿åŠ¿æ—¶ï¼ŒWiFiåŸºäºçš„æ¨¡å‹å­˜åœ¨åè§ï¼Œå¯èƒ½ä¼šäº§ç”Ÿé”™è¯¯çš„èº«ä½“éƒ¨åˆ†ï¼ˆè¯·å‚è§å›¾8ï¼ˆa-bï¼‰ä¸­çš„ç¤ºä¾‹ï¼‰ã€‚ï¼ˆ2ï¼‰å½“ä¸€æ¬¡æ•æ‰ä¸­å­˜åœ¨ä¸‰ä¸ªæˆ–ä¸‰ä¸ªä»¥ä¸Šçš„å¹¶å‘ä¸»é¢˜æ—¶ï¼ŒWiFiåŸºäºçš„æ¨¡å‹ä»æ•´ä¸ªæ•æ‰çš„å¹…å€¼å’Œç›¸ä½å¼ é‡ä¸­æå–æ¯ä¸ªä¸ªä½“çš„è¯¦ç»†ä¿¡æ¯æ›´å…·æŒ‘æˆ˜æ€§ï¼ˆè¯·å‚è§å›¾8ï¼ˆc-dï¼‰ä¸­çš„ç¤ºä¾‹ï¼‰ã€‚æˆ‘ä»¬ç›¸ä¿¡è¿™ä¸¤ä¸ªé—®é¢˜éƒ½å¯ä»¥é€šè¿‡è·å¾—æ›´å…¨é¢çš„è®­ç»ƒæ•°æ®æ¥è§£å†³ã€‚

# 5 CONCLUSION AND FUTURE WORK
In this paper, we demonstrated that it is possible to obtain dense human body poses from WiFi signals by utilizing deep learning architectures commonly used in computer vision. Instead of directly training a randomly initialized WiFi-based model, we explored rich supervision information to improve both the performance and training efficiency, such as utilizing the CSI phase, adding keypoint detection branch, and transfer learning from an image-based model. The performance of our work is still limited by the public training data in the field of WiFi-based perception, especially under different layouts. In future work, we also plan to collect multi-layout data and extend our work to predict 3D human body shapes from WiFi signals. We believe that the advanced capability of dense perception could empower the WiFi device as a privacy-friendly, illumination- invariant, and cheap human sensor compared to RGB cameras and Lidars.

> æœ¬æ–‡è¯æ˜äº†é€šè¿‡åˆ©ç”¨è®¡ç®—æœºè§†è§‰é¢†åŸŸä¸­å¸¸ç”¨çš„æ·±åº¦å­¦ä¹ æ¶æ„ï¼Œå¯ä»¥ä»WiFiä¿¡å·ä¸­è·å¾—å¯†é›†çš„äººä½“å§¿åŠ¿ã€‚æˆ‘ä»¬ä¸æ˜¯ç›´æ¥è®­ç»ƒä¸€ä¸ªéšæœºåˆå§‹åŒ–çš„WiFiæ¨¡å‹ï¼Œè€Œæ˜¯æ¢ç´¢äº†ä¸°å¯Œçš„ç›‘ç£ä¿¡æ¯æ¥æé«˜æ€§èƒ½å’Œè®­ç»ƒæ•ˆç‡ï¼Œä¾‹å¦‚åˆ©ç”¨CSIç›¸ä½ï¼Œæ·»åŠ å…³é”®ç‚¹æ£€æµ‹åˆ†æ”¯ï¼Œä»¥åŠä»åŸºäºå›¾åƒçš„æ¨¡å‹è¿›è¡Œè½¬ç§»å­¦ä¹ ã€‚æˆ‘ä»¬çš„å·¥ä½œä»ç„¶å—åˆ°WiFiæ„ŸçŸ¥é¢†åŸŸçš„å…¬å…±è®­ç»ƒæ•°æ®çš„é™åˆ¶ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸åŒçš„å¸ƒå±€ä¸‹ã€‚åœ¨æœªæ¥çš„å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è¿˜è®¡åˆ’æ”¶é›†å¤šå¸ƒå±€æ•°æ®ï¼Œå¹¶å°†æˆ‘ä»¬çš„å·¥ä½œæ‰©å±•åˆ°é¢„æµ‹WiFiä¿¡å·ä¸­çš„3Däººä½“å½¢çŠ¶ã€‚æˆ‘ä»¬ç›¸ä¿¡å¯†é›†æ„ŸçŸ¥çš„å…ˆè¿›èƒ½åŠ›å¯ä»¥ä½¿WiFiè®¾å¤‡æˆä¸ºéšç§å‹å¥½ï¼Œä¸å—ç…§æ˜å½±å“ï¼Œå¹¶ä¸”æ¯”RGBæ‘„åƒå¤´å’ŒLidarä¾¿å®œçš„äººä½“ä¼ æ„Ÿå™¨ã€‚

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/9fa64117a77049a0884c33e96d30483e.png#pic_center)
*Figure 8: Examples pf failure cases: (a-b) rare poses; (c-d) Three or more concurrent subjects. The first row is ground truth dense pose estimation. The second row illustrates the predicted dense pose.*

> *å›¾8ï¼šå¤±è´¥æ¡ˆä¾‹çš„ä¾‹å­ï¼š(a-b)ç½•è§çš„å§¿åŠ¿ï¼›(c-d)ä¸‰ä¸ªæˆ–æ›´å¤šåŒæ—¶å‡ºç°çš„ä¸»ä½“ã€‚ç¬¬ä¸€è¡Œæ˜¯åœ°é¢çœŸå®çš„å¯†é›†å§¿æ€ä¼°è®¡ã€‚ç¬¬äºŒè¡Œè¯´æ˜äº†é¢„æµ‹çš„å¯†é›†å§¿æ€ã€‚
![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/3e985f2389044ed5a6bfd3b89cb9932c.png#pic_center)
*Figure 9: Qualitative comparison using synchronized images and WiFi signals. (Left Column) image-based DensePose (Right Column) our WiFi-based DensePose.*

> *å›¾9ï¼šä½¿ç”¨åŒæ­¥å›¾åƒå’ŒWiFiä¿¡å·çš„å®šæ€§æ¯”è¾ƒã€‚(å·¦æ ï¼‰åŸºäºå›¾åƒçš„DensePoseï¼ˆå³æ ï¼‰æˆ‘ä»¬åŸºäºWiFiçš„DensePoseã€‚

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/10f2b9e5465b4f53890feb65cdb3bce8.png#pic_center)

*Figure 10: More qualitative comparison using synchronized images and WiFi signals. (Left Column) image-based DensePose (Right Column) our WiFi-based DensePose.*

> *å›¾10ï¼šä½¿ç”¨åŒæ­¥çš„å›¾åƒå’ŒWiFiä¿¡å·è¿›è¡Œæ›´å¤šçš„å®šæ€§æ¯”è¾ƒã€‚(å·¦æ ï¼‰åŸºäºå›¾åƒçš„DensePoseï¼ˆå³æ ï¼‰æˆ‘ä»¬åŸºäºWiFiçš„DensePoseã€‚

# REFERENCES

[1] Fadel Adib, Chen-Yu Hsu, Hongzi Mao, Dina Katabi, and FrÃ©do Durand. 2015. Capturing the Human Figure through a Wall. ACM Trans. Graph. 34, 6, Article 219 (oct 2015), 13 pages. https://doi.org/10.1145/2816795.2818072
[2] FadelAdib,ZachKabelac,DinaKatabi,andRobertC.Miller.2014.3DTrackingvia Body Radio Reflections. In 11th USENIX Symposium on Networked Systems Design andImplementation(NSDI14).USENIXAssociation,Seattle,WA,317â€“329. https: //www.usenix.org/conference/nsdi14/technical-sessions/presentation/adib
[3] Thiemo Alldieck, Gerard Pons-Moll, Christian Theobalt, and Marcus Magnor. 2019. Tex2Shape: Detailed Full Human Body Geometry From a Single Image. arXiv:1904.08645 [cs.CV]
[4] MykhayloAndriluka,UmarIqbal,EldarInsafutdinov,LeonidPishchulin,Anton Milan, Juergen Gall, and Bernt Schiele. 2018. PoseTrack: A Benchmark for Human Pose Estimation and Tracking. arXiv:1710.10000 [cs.CV]
[5] Chaimaa BASRI and Ahmed El Khadimi. 2016. Survey on indoor localization system and recent advances of WIFI fingerprinting technique. In International Conference on Multimedia Computing and Systems.
[6] Hilton Bristow, Jack Valmadre, and Simon Lucey. 2015. Dense Semantic Corre- spondence where Every Pixel is a Classifier. arXiv:1505.04143 [cs.CV]
[7] Zhe Cao, Gines Hidalgo, Tomas Simon, Shih-En Wei, and Yaser Sheikh. 2019. OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields. arXiv:1812.08008 [cs.CV]
[8] Riza Alp GÃ¼ler, Natalia Neverova, and Iasonas Kokkinos. 2018. DensePose: Dense Human Pose Estimation In The Wild. CoRR abs/1802.00434 (2018). arXiv:1802.00434 http://arxiv.org/abs/1802.00434
[9] Kaiming He, Georgia Gkioxari, Piotr DollÃ¡r, and Ross B. Girshick. 2017. Mask R-CNN. CoRR abs/1703.06870 (2017). arXiv:1703.06870 http://arxiv.org/abs/1703. 06870
[10] Weipeng Jiang, Yongjun Liu, Yun Lei, Kaiyao Wang, Hui Yang, and Zhihao Xing. 2017. For Better CSI Fingerprinting Based Localization: A Novel Phase Sanitization Method and a Distance Metric. In 2017 IEEE 85th Vehicular Technology Conference(VTCSpring).1â€“7. https://doi.org/10.1109/VTCSpring.2017.8108351
[11] Mohammad Hadi Kefayati, Vahid Pourahmadi, and Hassan Aghaeinia. 2020. Wi2Vi: Generating Video Frames from WiFi CSI Samples. CoRR abs/2001.05842 (2020). arXiv:2001.05842 https://arxiv.org/abs/2001.05842
[12] Muhammed Kocabas, Nikos Athanasiou, and Michael J. Black. 2020. VIBE: Video Inference for Human Body Pose and Shape Estimation. arXiv:1912.05656 [cs.CV]
[13] Anton Ledergerber and Raffaello Dâ€™Andrea. 2019. Ultra-Wideband Angle of Ar- rival Estimation Based on Angle-Dependent Antenna Transfer Function. Sensors 19, 20 (2019). https://doi.org/10.3390/s19204466
[14] Tsung-YiLin,PiotrDollÃ¡r,RossB.Girshick,KaimingHe,BharathHariharan,and Serge J. Belongie. 2016. Feature Pyramid Networks for Object Detection. CoRR
abs/1612.03144 (2016). arXiv:1612.03144 http://arxiv.org/abs/1612.03144
[15] Tsung-YiLin,MichaelMaire,SergeJ.Belongie,LubomirD.Bourdev,RossB.Gir- shick, James Hays, Pietro Perona, Deva Ramanan, Piotr DollÃ¡r, and C. Lawrence
Zitnick. 2014. Microsoft COCO: Common Objects in Context. CoRR abs/1405.0312
[16] Jonathan Long, Evan Shelhamer, and Trevor Darrell. 2014. Fully Convo- lutional Networks for Semantic Segmentation. CoRR abs/1411.4038 (2014). arXiv:1411.4038 http://arxiv.org/abs/1411.4038
[17] Daniel Maturana and Sebastian Scherer. 2015. VoxNet: A 3D Convolutional Neural Network for real-time object recognition. In 2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). 922â€“928. https://doi.org/10. 1109/IROS.2015.7353481
[18] NataliaNeverova,DavidNovotny,VasilKhalidov,MarcSzafraniec,PatrickLa- batut, and Andrea Vedaldi. 2020. Continuous Surface Embeddings. Advances in Neural Information Processing Systems.
[19] Joseph Redmon and Ali Farhadi. 2018. YOLOv3: An Incremental Improvement. arXiv:1804.02767 [cs.CV]
[20] Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun. 2015. Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks. CoRR abs/1506.01497 (2015). arXiv:1506.01497 http://arxiv.org/abs/1506.01497
[21] Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun. 2015. Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks. CoRR abs/1506.01497 (2015). arXiv:1506.01497 http://arxiv.org/abs/1506.01497
[22] Shunsuke Saito, , Zeng Huang, Ryota Natsume, Shigeo Morishima, Angjoo Kanazawa, and Hao Li. 2019. PIFu: Pixel-Aligned Implicit Function for High- Resolution Clothed Human Digitization. arXiv preprint arXiv:1905.05172 (2019).
[23] SouvikSen,BoÅ¾idarRadunovic,RomitRoyChoudhury,andTomMinka.2012. You Are Facing the Mona Lisa: Spot Localization Using PHY Layer Information. In Proceedings of the 10th International Conference on Mobile Systems, Applications, and Services (Low Wood Bay, Lake District, UK) (MobiSys â€™12). Association for Computing Machinery, New York, NY, USA, 183â€“196. https://doi.org/10.1145/ 2307636.2307654
[24] RomanShapovalov,DavidNovotnÃ½,BenjaminGraham,PatrickLabatut,andAn- drea Vedaldi. 2021. DensePose 3D: Lifting Canonical Surface Maps of Articulated Objects to the Third Dimension. CoRR abs/2109.00033 (2021). arXiv:2109.00033 https://arxiv.org/abs/2109.00033
[25] B. Sheng, F. Xiao, L. Sha, and L. Sun. 2020. Deep Spatialâ€“Temporal Model Based Cross-Scene Action Recognition Using Commodity WiFi. IEEE Internet of Things Journal 7 (2020).
[26] Elahe Soltanaghaei, Avinash Kalyanaraman, and Kamin Whitehouse. 2018. Mul- tipath Triangulation: Decimeter-Level WiFi Localization and Orientation with a Single Unaided Receiver. In Proceedings of the 16th Annual International Con- ference on Mobile Systems, Applications, and Services (Munich, Germany) (Mo- biSys â€™18). Association for Computing Machinery, New York, NY, USA, 376â€“388. https://doi.org/10.1145/3210240.3210347
[27] Elahe Soltanaghaei, Avinash Kalyanaraman, and Kamin Whitehouse. 2018. Mul- tipath Triangulation: Decimeter-level WiFi Localization and Orientation with a Single Unaided Receiver. In Proceedings of the 16th Annual International Confer- ence on Mobile Systems, Applications, and Services.
[28] YuSun,QianBao,WuLiu,YiliFu,MichaelJ.Black,andTaoMei.2021.Monocular, One-stage, Regression of Multiple 3D People. arXiv:2008.12272 [cs.CV]
[29] FeiWang,YunpengSong,JimuyangZhang,JinsongHan,andDongHuang.2019. Temporal Unet: Sample Level Human Action Recognition using WiFi. arXiv Preprint arXiv:1904.11953 (2019).
[30] FeiWang,SanpingZhou,StanislavPanev,JinsongHan,andDongHuang.2019. Person-in-WiFi: Fine-grained Person Perception using WiFi. CoRR abs/1904.00276 (2019). arXiv:1904.00276 http://arxiv.org/abs/1904.00276
[31] Fei Wang, Sanping Zhou, Stanislav Panev, Jinsong Han, and Dong Huang. 2019. Person-in-WiFi: Fine-grained Person Perception using WiFi. In ICCV.
[32] Zhe Wang, Yang Liu, Qinghai Liao, Haoyang Ye, Ming Liu, and Lujia Wang. 2018. Characterization of a RS-LiDAR for 3D Perception. In 2018 IEEE 8th Annual International Conference on CYBER Technology in Automation, Control, and Intelligent Systems (CYBER). 564â€“569. https://doi.org/10.1109/CYBER.2018.8688235
[33] Shih-En Wei, Varun Ramakrishna, Takeo Kanade, and Yaser Sheikh. 2016. Convolutional Pose Machines. arXiv:1602.00134 [cs.CV]
[34] Bin Xiao, Haiping Wu, and Yichen Wei. 2018. Simple Baselines for Human Pose Estimation and Tracking. arXiv:1804.06208 [cs.CV]
[35] Yuanlu Xu, Song-Chun Zhu, and Tony Tung. 2019. DenseRaC: Joint 3D Pose and Shape Estimation by Dense Render-and-Compare. CoRR abs/1910.00116 (2019). arXiv:1910.00116 http://arxiv.org/abs/1910.00116
[36] Hongfei Xue, Yan Ju, Chenglin Miao, Yijiang Wang, Shiyang Wang, Aidong Zhang, and Lu Su. 2021. mmMesh: towards 3D real-time dynamic human mesh con- struction using millimeter-wave. In Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services. 269â€“282.
[37] Pengfei Yao, Zheng Fang, Fan Wu, Yao Feng, and Jiwei Li. 2019. DenseBody: Directly Regressing Dense 3D Human Pose and Shape From a Single Color Image. arXiv:1903.10153 [cs.CV]
[38] Hongwen Zhang, Jie Cao, Guo Lu, Wanli Ouyang, and Zhenan Sun. 2020. Learning 3D Human Shape and Pose from Dense Body Parts. arXiv:1912.13344 [cs.CV]
[39] Mingmin Zhao, Tianhong Li, Mohammad Abu Alsheikh, Yonglong Tian, Hang Zhao, Antonio Torralba, and Dina Katabi. 2018. Through-Wall Human Pose Estimation Using Radio Signals. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. 7356â€“7365. https://doi.org/10.1109/CVPR.2018.00768
[40] Tinghui Zhou, Philipp KrÃ¤henbÃ¼hl, Mathieu Aubry, Qixing Huang, and Alexei A. Efros. 2016. Learning Dense Correspondence via 3D-guided Cycle Consistency. arXiv:1604.05383 [cs.CV]
